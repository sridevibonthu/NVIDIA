{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/architecture.html#section-concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html#section-dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 41.6667 infer/sec. Avg latency: 24171 usec (std 188 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 24192 usec (std 88 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 24236 usec (std 65 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 24236 usec (standard deviation 65 usec)\n",
      "    p50 latency: 24236 usec\n",
      "    p90 latency: 24316 usec\n",
      "    p95 latency: 24359 usec\n",
      "    p99 latency: 24426 usec\n",
      "    Avg HTTP time: 24230 usec (send 5 usec + response wait 24224 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 23924 usec (overhead 4 usec + queue 30 usec + compute input 11 usec + compute infer 23868 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 48196 usec (std 184 usec)\n",
      "  Pass [2] throughput: 41.6667 infer/sec. Avg latency: 48225 usec (std 212 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 48298 usec (std 221 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 48298 usec (standard deviation 221 usec)\n",
      "    p50 latency: 48284 usec\n",
      "    p90 latency: 48541 usec\n",
      "    p95 latency: 48755 usec\n",
      "    p99 latency: 48927 usec\n",
      "    Avg HTTP time: 48312 usec (send 5 usec + response wait 48306 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 150\n",
      "    Execution count: 150\n",
      "    Successful request count: 150\n",
      "    Avg request latency: 47969 usec (overhead 3 usec + queue 23894 usec + compute input 11 usec + compute infer 24050 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 72630 usec (std 314 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 72668 usec (std 287 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 72580 usec (std 216 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 72580 usec (standard deviation 216 usec)\n",
      "    p50 latency: 72590 usec\n",
      "    p90 latency: 72852 usec\n",
      "    p95 latency: 72913 usec\n",
      "    p99 latency: 73115 usec\n",
      "    Avg HTTP time: 72616 usec (send 5 usec + response wait 72610 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 72244 usec (overhead 3 usec + queue 48119 usec + compute input 10 usec + compute infer 24101 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 96788 usec (std 332 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 96850 usec (std 226 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 96970 usec (std 292 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 96970 usec (standard deviation 292 usec)\n",
      "    p50 latency: 96902 usec\n",
      "    p90 latency: 97429 usec\n",
      "    p95 latency: 97618 usec\n",
      "    p99 latency: 97741 usec\n",
      "    Avg HTTP time: 96976 usec (send 6 usec + response wait 96969 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 96606 usec (overhead 3 usec + queue 72442 usec + compute input 12 usec + compute infer 24138 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 121771 usec (std 1100 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 121213 usec (std 381 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 121316 usec (std 391 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 121316 usec (standard deviation 391 usec)\n",
      "    p50 latency: 121194 usec\n",
      "    p90 latency: 121880 usec\n",
      "    p95 latency: 122007 usec\n",
      "    p99 latency: 122426 usec\n",
      "    Avg HTTP time: 121279 usec (send 5 usec + response wait 121273 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 120908 usec (overhead 3 usec + queue 96732 usec + compute input 10 usec + compute infer 24152 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 145729 usec (std 446 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 146042 usec (std 471 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 146119 usec (std 324 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 146119 usec (standard deviation 324 usec)\n",
      "    p50 latency: 146150 usec\n",
      "    p90 latency: 146515 usec\n",
      "    p95 latency: 146618 usec\n",
      "    p99 latency: 146941 usec\n",
      "    Avg HTTP time: 146073 usec (send 6 usec + response wait 146066 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 145691 usec (overhead 4 usec + queue 121429 usec + compute input 11 usec + compute infer 24235 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 170594 usec (std 346 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 170567 usec (std 374 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 170897 usec (std 442 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 170897 usec (standard deviation 442 usec)\n",
      "    p50 latency: 170799 usec\n",
      "    p90 latency: 171612 usec\n",
      "    p95 latency: 171735 usec\n",
      "    p99 latency: 171822 usec\n",
      "    Avg HTTP time: 170873 usec (send 7 usec + response wait 170864 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 170474 usec (overhead 3 usec + queue 146154 usec + compute input 12 usec + compute infer 24293 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 40.6667 infer/sec. Avg latency: 195306 usec (std 388 usec)\n",
      "  Pass [2] throughput: 40.6667 infer/sec. Avg latency: 195168 usec (std 297 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 195381 usec (std 395 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 195381 usec (standard deviation 395 usec)\n",
      "    p50 latency: 195332 usec\n",
      "    p90 latency: 195915 usec\n",
      "    p95 latency: 196073 usec\n",
      "    p99 latency: 196600 usec\n",
      "    Avg HTTP time: 195360 usec (send 5 usec + response wait 195354 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 147\n",
      "    Execution count: 147\n",
      "    Successful request count: 147\n",
      "    Avg request latency: 194991 usec (overhead 2 usec + queue 170653 usec + compute input 11 usec + compute infer 24314 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 219907 usec (std 1502 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 219888 usec (std 346 usec)\n",
      "  Pass [3] throughput: 40.6667 infer/sec. Avg latency: 220156 usec (std 300 usec)\n",
      "  Client: \n",
      "    Request count: 122\n",
      "    Throughput: 40.6667 infer/sec\n",
      "    Avg latency: 220156 usec (standard deviation 300 usec)\n",
      "    p50 latency: 220188 usec\n",
      "    p90 latency: 220514 usec\n",
      "    p95 latency: 220593 usec\n",
      "    p99 latency: 220754 usec\n",
      "    Avg HTTP time: 220231 usec (send 7 usec + response wait 220223 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 147\n",
      "    Execution count: 147\n",
      "    Successful request count: 147\n",
      "    Avg request latency: 219844 usec (overhead 2 usec + queue 195453 usec + compute input 12 usec + compute infer 24365 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 40.6667 infer/sec. Avg latency: 244408 usec (std 2526 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 245360 usec (std 765 usec)\n",
      "  Pass [3] throughput: 40.6667 infer/sec. Avg latency: 245801 usec (std 916 usec)\n",
      "  Client: \n",
      "    Request count: 122\n",
      "    Throughput: 40.6667 infer/sec\n",
      "    Avg latency: 245801 usec (standard deviation 916 usec)\n",
      "    p50 latency: 245718 usec\n",
      "    p90 latency: 247259 usec\n",
      "    p95 latency: 247658 usec\n",
      "    p99 latency: 247849 usec\n",
      "    Avg HTTP time: 245722 usec (send 9 usec + response wait 245711 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 146\n",
      "    Execution count: 146\n",
      "    Successful request count: 146\n",
      "    Avg request latency: 245292 usec (overhead 3 usec + queue 220820 usec + compute input 14 usec + compute infer 24440 usec + compute output 15 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 41.3333 infer/sec, latency 24236 usec\n",
      "Concurrency: 2, throughput: 41.3333 infer/sec, latency 48298 usec\n",
      "Concurrency: 3, throughput: 41.3333 infer/sec, latency 72580 usec\n",
      "Concurrency: 4, throughput: 41 infer/sec, latency 96970 usec\n",
      "Concurrency: 5, throughput: 41.3333 infer/sec, latency 121316 usec\n",
      "Concurrency: 6, throughput: 41.3333 infer/sec, latency 146119 usec\n",
      "Concurrency: 7, throughput: 41 infer/sec, latency 170897 usec\n",
      "Concurrency: 8, throughput: 41 infer/sec, latency 195381 usec\n",
      "Concurrency: 9, throughput: 40.6667 infer/sec, latency 220156 usec\n",
      "Concurrency: 10, throughput: 40.6667 infer/sec, latency 245801 usec\n"
     ]
    }
   ],
   "source": [
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have observed utilization similar to the following:<br/>\n",
    "<img src=\"images/NVIDIASMI2.png\" width=800/><br/>\n",
    "\n",
    "Do you think you will observe a major acceleration as a consequence of increasing the number of instance groups?<br>\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Implementation\n",
    "Let's look at how to enable concurrent execution and what impact it will have on our model performance. Execute the following code cells to export the model in the ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.41034746170043945 seconds\n",
      "time of error check of onnx model:  21.00487446784973 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K Jun  9 12:54 .\n",
      "drwxr-xr-x 3 root root 4.0K Jun  9 12:53 ..\n",
      "drwxr-xr-x 2 root root 4.0K Jun  9 12:53 1\n",
      "-rw-r--r-- 1 root root  569 Jun  9 12:54 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Exercise: Configure Multiple Instance Groups\n",
    "In order to specify multiple instances, we need to change the \"count\" value from '1' to a larger number in the `instance_group` section of the \"config.pbtxt\" configuration file. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt) in the `bertQA-onnx-conexec` deployment just created to specify two instances of our BERT-based question answering model. You should find the default instance_group block at the end of the file. Change the count variable from 1 to 2.  (see the [solution](solutions/ex-2-1-3_config.pbtxt) as needed)\n",
    "2. To make the comparison fair, also enable TensorRT with the addition of an `execution_accelerators` block inside the `optimization` block:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Once you have saved your changes (Main menu: File -> Save File), move the model across to Triton by executing the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run our standard stress test against the model. Please compare it to the single instance execution.<br>\n",
    "   Did the throughput change?<br>\n",
    "   Did the latency change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 24293 usec (std 94 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 24337 usec (std 105 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 24380 usec (std 117 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 24380 usec (standard deviation 117 usec)\n",
      "    p50 latency: 24352 usec\n",
      "    p90 latency: 24549 usec\n",
      "    p95 latency: 24597 usec\n",
      "    p99 latency: 24697 usec\n",
      "    Avg HTTP time: 24369 usec (send 5 usec + response wait 24363 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 24048 usec (overhead 3 usec + queue 27 usec + compute input 11 usec + compute infer 23996 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 48340 usec (std 134 usec)\n",
      "  Pass [2] throughput: 41.6667 infer/sec. Avg latency: 48273 usec (std 124 usec)\n",
      "  Pass [3] throughput: 41.6667 infer/sec. Avg latency: 48330 usec (std 215 usec)\n",
      "  Client: \n",
      "    Request count: 125\n",
      "    Throughput: 41.6667 infer/sec\n",
      "    Avg latency: 48330 usec (standard deviation 215 usec)\n",
      "    p50 latency: 48290 usec\n",
      "    p90 latency: 48495 usec\n",
      "    p95 latency: 48648 usec\n",
      "    p99 latency: 49350 usec\n",
      "    Avg HTTP time: 48334 usec (send 5 usec + response wait 48328 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 47986 usec (overhead 3 usec + queue 23898 usec + compute input 11 usec + compute infer 24063 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 41.6667 infer/sec. Avg latency: 72513 usec (std 171 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 72481 usec (std 168 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 72481 usec (std 158 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 72481 usec (standard deviation 158 usec)\n",
      "    p50 latency: 72450 usec\n",
      "    p90 latency: 72730 usec\n",
      "    p95 latency: 72790 usec\n",
      "    p99 latency: 72862 usec\n",
      "    Avg HTTP time: 72454 usec (send 6 usec + response wait 72447 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 72091 usec (overhead 2 usec + queue 48017 usec + compute input 11 usec + compute infer 24050 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 96609 usec (std 245 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 97100 usec (std 471 usec)\n",
      "  Pass [3] throughput: 41.6667 infer/sec. Avg latency: 96706 usec (std 339 usec)\n",
      "  Client: \n",
      "    Request count: 125\n",
      "    Throughput: 41.6667 infer/sec\n",
      "    Avg latency: 96706 usec (standard deviation 339 usec)\n",
      "    p50 latency: 96625 usec\n",
      "    p90 latency: 97182 usec\n",
      "    p95 latency: 97372 usec\n",
      "    p99 latency: 97968 usec\n",
      "    Avg HTTP time: 96730 usec (send 7 usec + response wait 96722 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 149\n",
      "    Execution count: 149\n",
      "    Successful request count: 149\n",
      "    Avg request latency: 96333 usec (overhead 3 usec + queue 72234 usec + compute input 12 usec + compute infer 24073 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 121042 usec (std 397 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 121584 usec (std 593 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 121306 usec (std 419 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 121306 usec (standard deviation 419 usec)\n",
      "    p50 latency: 121185 usec\n",
      "    p90 latency: 121888 usec\n",
      "    p95 latency: 122242 usec\n",
      "    p99 latency: 122509 usec\n",
      "    Avg HTTP time: 121270 usec (send 9 usec + response wait 121260 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 120865 usec (overhead 3 usec + queue 96696 usec + compute input 13 usec + compute infer 24141 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 145665 usec (std 338 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 145780 usec (std 440 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 145799 usec (std 279 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 145799 usec (standard deviation 279 usec)\n",
      "    p50 latency: 145796 usec\n",
      "    p90 latency: 146156 usec\n",
      "    p95 latency: 146265 usec\n",
      "    p99 latency: 146401 usec\n",
      "    Avg HTTP time: 145856 usec (send 5 usec + response wait 145850 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 145487 usec (overhead 3 usec + queue 121261 usec + compute input 10 usec + compute infer 24202 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 41.3333 infer/sec. Avg latency: 170383 usec (std 272 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 170367 usec (std 255 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 170562 usec (std 434 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 170562 usec (standard deviation 434 usec)\n",
      "    p50 latency: 170498 usec\n",
      "    p90 latency: 171016 usec\n",
      "    p95 latency: 171484 usec\n",
      "    p99 latency: 172102 usec\n",
      "    Avg HTTP time: 170643 usec (send 7 usec + response wait 170635 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 147\n",
      "    Execution count: 147\n",
      "    Successful request count: 147\n",
      "    Avg request latency: 170255 usec (overhead 3 usec + queue 145958 usec + compute input 12 usec + compute infer 24271 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 195098 usec (std 410 usec)\n",
      "  Pass [2] throughput: 41.3333 infer/sec. Avg latency: 194907 usec (std 372 usec)\n",
      "  Pass [3] throughput: 41.3333 infer/sec. Avg latency: 195110 usec (std 326 usec)\n",
      "  Client: \n",
      "    Request count: 124\n",
      "    Throughput: 41.3333 infer/sec\n",
      "    Avg latency: 195110 usec (standard deviation 326 usec)\n",
      "    p50 latency: 195068 usec\n",
      "    p90 latency: 195646 usec\n",
      "    p95 latency: 195787 usec\n",
      "    p99 latency: 196002 usec\n",
      "    Avg HTTP time: 195078 usec (send 5 usec + response wait 195072 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 194710 usec (overhead 3 usec + queue 170410 usec + compute input 11 usec + compute infer 24275 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 41 infer/sec. Avg latency: 219662 usec (std 550 usec)\n",
      "  Pass [2] throughput: 40.6667 infer/sec. Avg latency: 219990 usec (std 491 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 219888 usec (std 492 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 219888 usec (standard deviation 492 usec)\n",
      "    p50 latency: 219780 usec\n",
      "    p90 latency: 220635 usec\n",
      "    p95 latency: 220791 usec\n",
      "    p99 latency: 221007 usec\n",
      "    Avg HTTP time: 219890 usec (send 7 usec + response wait 219882 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 148\n",
      "    Execution count: 148\n",
      "    Successful request count: 148\n",
      "    Avg request latency: 219502 usec (overhead 3 usec + queue 195153 usec + compute input 12 usec + compute infer 24323 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 40.6667 infer/sec. Avg latency: 245294 usec (std 2641 usec)\n",
      "  Pass [2] throughput: 41 infer/sec. Avg latency: 244984 usec (std 414 usec)\n",
      "  Pass [3] throughput: 41 infer/sec. Avg latency: 244988 usec (std 360 usec)\n",
      "  Client: \n",
      "    Request count: 123\n",
      "    Throughput: 41 infer/sec\n",
      "    Avg latency: 244988 usec (standard deviation 360 usec)\n",
      "    p50 latency: 244956 usec\n",
      "    p90 latency: 245469 usec\n",
      "    p95 latency: 245567 usec\n",
      "    p99 latency: 245717 usec\n",
      "    Avg HTTP time: 244924 usec (send 6 usec + response wait 244917 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 147\n",
      "    Execution count: 147\n",
      "    Successful request count: 147\n",
      "    Avg request latency: 244543 usec (overhead 3 usec + queue 220136 usec + compute input 11 usec + compute infer 24382 usec + compute output 11 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 41 infer/sec, latency 24380 usec\n",
      "Concurrency: 2, throughput: 41.6667 infer/sec, latency 48330 usec\n",
      "Concurrency: 3, throughput: 41.3333 infer/sec, latency 72481 usec\n",
      "Concurrency: 4, throughput: 41.6667 infer/sec, latency 96706 usec\n",
      "Concurrency: 5, throughput: 41 infer/sec, latency 121306 usec\n",
      "Concurrency: 6, throughput: 41.3333 infer/sec, latency 145799 usec\n",
      "Concurrency: 7, throughput: 41 infer/sec, latency 170562 usec\n",
      "Concurrency: 8, throughput: 41.3333 infer/sec, latency 195110 usec\n",
      "Concurrency: 9, throughput: 41 infer/sec, latency 219888 usec\n",
      "Concurrency: 10, throughput: 41 infer/sec, latency 244988 usec\n"
     ]
    }
   ],
   "source": [
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's free up some GPU memory by moving some of the models out of the Triton model repository.  After removing the following three models, only the `bertQA-torchscript` model should remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertQA-torchscript\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scheduling Strategies\n",
    "Triton supports batch inferencing by allowing individual inference requests to specify a batch of inputs. The inferencing for a batch of inputs is performed at the same time which is especially important for GPUs since it can greatly increase inferencing throughput. In many use cases the individual inference requests are not batched, therefore, they do not benefit from the throughput benefits of batching. <br/>\n",
    "\n",
    "The inference server contains multiple scheduling and batching algorithms that support many different model types and use-cases. The choice of the scheduler / batcher will be driven by several factors the key ones being:\n",
    "- Stateful / stateless nature of your inference workload\n",
    "- Whether your application is composed of models served in isolation or whether a more complex pipeline / ensemble is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Stateless Inference\n",
    "\n",
    "When dealing with stateless inference (as we are in this class) we have two main options when it comes to scheduling. The first option is the default scheduler which will distribute request to all instances assigned for inference. This is the preferred option when the structure of the inference workload is well understood and where inference will take place at regular batch sizes and time intervals.\n",
    "\n",
    "The second option is dynamic batching which combines individual request and similarly to the default batcher distributes the larges batches across instances. We will discuss this particular option in the next section of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Stateful Inference\n",
    "\n",
    "A stateful model (or stateful custom backend) does maintain state between inference requests. The model is expecting multiple inference requests that together form a sequence of inferences that must be routed to the same model instance so that the state being maintained by the model is correctly updated. Moreover, the model may require that Triton provide control signals indicating, for example, sequence start.\n",
    "\n",
    "The sequence batcher can employ one of two scheduling strategies when deciding how to batch the sequences that are routed to the same model instance. These strategies are Direct and Oldest.\n",
    "\n",
    "With the Direct scheduling strategy the sequence batcher ensures not only that all inference requests in a sequence are routed to the same model instance, but also that each sequence is routed to a dedicated batch slot within the model instance. This strategy is required when the model maintains state for each batch slot, and is expecting all inference requests for a given sequence to be routed to the same slot so that the state is correctly updated.\n",
    "\n",
    "With the Oldest scheduling strategy the sequence batcher ensures that all inference requests in a sequence are routed to the same model instance and then uses the dynamic batcher to batch together multiple inferences from different sequences into a batch that inferences together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Pipelines / Ensembles\n",
    "\n",
    "An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data post-processing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton. An example of an ensemble pipeline is illustrated below: <br/>\n",
    "\n",
    "<img src=\"images/ensemble_example0.png\"/>\n",
    "\n",
    "The ensemble scheduler must be used for ensemble models, regardless of the scheduler used by the models within the ensemble. With respect to the ensemble scheduler, an ensemble model is not an actual model. Instead, it specifies the data flow between models within the ensemble as Step. The scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification. In spite of that, the ensemble model is still viewed as a single model from an external view.\n",
    "\n",
    "More information on Triton scheduling can be found in the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html#stateless-models\">following section of the documentation</a>. In this class, we will focus further on one of the most powerful features of Triton, *dynamic batching*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Dynamic Batching\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically, resulting in increased throughput.\n",
    "\n",
    "When a model instance becomes available for inferencing, the dynamic batcher will attempt to create batches from the requests that are available in the scheduler. Requests are added to the batch in the order the requests were received. If the dynamic batcher can form a batch of a preferred size(s) it will create a batch of the largest possible preferred size and send it for inferencing. If the dynamic batcher cannot form a batch of a preferred size, it will send a batch of the largest size possible that is less than the max batch size allowed by the model. \n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Exercise: Implement Dynamic Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin again by exporting an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4355282783508301 seconds\n",
      "time of error check of onnx model:  17.662055492401123 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt) for dynamic batching using the example snippet. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. Enable TensorRT in the optimization block.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. Once saved, move the model to the Triton model repository and run the performance utility by executing the following cells. ([solution](solutions/ex-2-3-1_config.pbtxt) if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have observed a fairly dramatic improvement in both latency and throughput. \n",
    "* How big is the impact in comparison to vanilla ONNX configuration or vanilla TorchScript? \n",
    "* What do you think was bottlenecking the multiple instance implementation?\n",
    "\n",
    "Discuss the results with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've leaned some strategies to improve the GPU utilization and reduce latency using:\n",
    "\n",
    "* Concurrent model execution\n",
    "* Scheduling\n",
    "* Dynamic batching\n",
    "\n",
    "In the next segment of the class we will make a more formal assessment of inference performance across multiple concurrency levels and how to analyze your inference performance in a structured way. Please proceed to the next notebook:<br>\n",
    "[3.0 Server Performance](030_ServerPerformance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
