{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Build a 3-Class Text Classifier\n",
    "\n",
    "In this notebook, you'll build an application to classify medical disease abstracts into one of three categories: cancer diseases, neurological diseases and disorders, and \"other\" for anything else.\n",
    "\n",
    "**[3.1 Set Up the Project](#3.1-Set-Up-the-Project)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1 Input Parameters](#3.1.1-Input-Parameters)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2 Prepare the Input Data](#3.1.2-Prepare-the-Input-Data)<br>\n",
    "**[3.2 Exercise: Create Neural Modules](#3.2-Exercise:-Create-Neural-Modules)**<br>\n",
    "**[3.3 Create Neural Graphs](#3.3-Create-Neural-Graphs)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.1 Training Graph](#3.3.1-Training-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.2 Exercise: Create the Validation Graph](#3.3.2-Exercise:-Create-the-Validation-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.3 Visualize Embeddings Before Training](#3.3.3-Visualize-Embeddings-Before-Training)<br>\n",
    "**[3.4 Training](#3.4-Training)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.1 Set the Learning Rate and Optimizer](#3.4.1-Set-the-Learning-Rate-and-Optimizer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.2 Create the Callbacks](#3.4.2-Create-the-Callbacks)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.3 Run the Trainer](#3.4.3-Run-the-Trainer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.4.4 Visualize Embeddings After Training](#3.4.4-Visualize-Embeddings-After-Training)<br>\n",
    "**[3.5 Inference](#3.5-Inference)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.1 Exercise: Create the Test Graph](#3.5.1-Exercise:-Create-the-Test-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.2 Run Inference on the Test Set](#3.5.2-Run-Inference-on-the-Test-Set)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.3 Inference Results](#3.5.3-Inference-Results)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.5.4 Single Sentence Classification](#3.5.4-Single-Sentence-Classification)<br>\n",
    "**[3.6 Exercise: Change the Language Model](#3.6-Exercise:-Change-the-Language-Model)**<br>\n",
    "\n",
    "As presented in the [1.0 Explore the Data](010_ExploreData.ipynb) notebook, the data for the project includes training and evaluation sets already labeled, and a test set with no labels, which will need to be fixed. You'll follow the steps outlined in the [2.0 NLP Projects with NeMo](020_ExploreNeMo.ipynb) to build your application, train it, and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Set Up the Project\n",
    "\n",
    "You've learned the general paradigm for creating NLP tasks with NVIDIA NeMo.  Start by importing all the specific libraries, objects, and functions you'll need for text classification using a pre-trained language model.\n",
    "\n",
    "<img src=\"../images/nemo/nm-pipe.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-06-08 15:58:20 audio_preprocessing:56] Could not import torchaudio. Some features might not work.\n"
     ]
    }
   ],
   "source": [
    "# Import useful math and utility libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = -1\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Import the nemo toolkit and NLP libraries\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "\n",
    "# Import the specific neural modules we need\n",
    "from nemo.collections.nlp.nm.data_layers import BertTextClassificationDataLayer\n",
    "from nemo.collections.nlp.nm.trainables import get_pretrained_lm_model, SequenceClassifier\n",
    "from nemo.backends.pytorch.common.losses import CrossEntropyLossNM\n",
    "    \n",
    "# Import helpers for fetching the learning rate and tokenizer functions\n",
    "from nemo.utils.lr_policies import get_lr_policy\n",
    "from nemo.collections.nlp.data.tokenizers import get_tokenizer\n",
    "\n",
    "# Import callbacks and callback functions\n",
    "from nemo.core import SimpleLogger, TensorboardLogger, EvaluatorCallback, CheckpointCallback\n",
    "from nemo.collections.nlp.callbacks.text_classification_callback import eval_iter_callback, eval_epochs_done_callback\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Input Parameters\n",
    "For reusability and convenience, set up all of the parameters in one place at the beginning of the notebook.  This way, it will be easy to make changes when you want to try alternate models and configurations. \n",
    "\n",
    "### Model Choice\n",
    "There are lots of models to choose from. Recall that the pretrained language models we are starting from were trained in a self-supervised manner over general text.  The models vary in size, and thus in memory and processing requirements. In addition, some were trained on \"uncased\" data, where all capital letters have been changed to lower case.  This results in a smaller vocabulary file, and is generally fine for many tasks, but may not be best for tasks where casing matters, such as NER.  These are some of the tradeoffs you should consider in choosing your model.  The table below gives statistics for a few of the NeMo pretrained models:\n",
    "\n",
    "| Model Name                 | Pretraining                                              | Size            |\n",
    "|----------------------------|----------------------------------------------------------|-----------------|\n",
    "| bert-base-uncased          | uncased Wikipedia and BookCorpus                         | 110M parameters |\n",
    "| bert-base-cased            | cased Wikipedia and BookCorpus                           | 110M parameters |\n",
    "| bert-large-uncased         | uncased Wikipedia and BookCorpus                         | 335M parameters |\n",
    "| bert-large-cased           | cased Wikipedia and BookCorpus                           | 335M parameters |\n",
    "| megatron-bert-345m-uncased | uncased Wikipedia, RealNews, OpenWebText, and CC-Stories | 335M parameters |\n",
    "| megatron-bert-345m-cased   | cased Wikipedia, RealNews, OpenWebText, and CC-Stories   | 335M parameters |\n",
    "\n",
    "For more discussion on pretrained models and their parameters, please see the blog post, [\"State-of-the-Art Language Modeling Using Megatron on the NVIDIA A100 GPU\"](https://developer.nvidia.com/blog/language-modeling-using-megatron-a100-gpu/).<br>\n",
    "The [AMP_OPTIMIZATION_LEVEL](https://nvidia.github.io/apex/amp.html#opt-levels) parameter can be used to set the mixed precision level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the input data location.\n",
    "DATA_DIR = '/dli/task/data/NCBI_tc-3/'\n",
    "\n",
    "# Identify the pretrained model and where information and checkpoints will be logged\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "WORK_DIR = '/dli/task/data/logs-tc-bbu/'\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = 'megatron-bert-345m-uncased'\n",
    "# WORK_DIR = '/dli/task/data/logs-tc-m345u/'\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = 'bert-large-uncased'\n",
    "# WORK_DIR = '/dli/task/data/logs-tc-blu/'\n",
    "\n",
    "# To use mixed precision, set AMP_OPTIMIZATION_LEVEL to 'O1' or 'O2',\n",
    "# to train without mixed precision, set it to 'O0'.\n",
    "AMP_OPTIMIZATION_LEVEL = 'O1'\n",
    "\n",
    "# Set the number of words in the sequences\n",
    "# Shorter sequences will be padded with 0s, longer ones truncated\n",
    "MAX_SEQ_LEN = 128 \n",
    "# set batch size - will need smaller 16 for 'bert-large-uncased' and 'megatron-bert-345m-uncased'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Prepare the Input Data\n",
    "In order to take advantage of NeMo's pre-built sentence classification data layer, the data should be formatted as \"sentence\\tlabel\" (sentence tab label).\n",
    "\n",
    "The training and evaluation datasets already have the correct \"sentence\\tlabel\" format and are ready to go for training, but the test set has no labels and needs to be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.4M\n",
      "-rw-r--r-- 1 702112 10513 133K Jul 21  2020 dev.tsv\n",
      "-rw-r--r-- 1 702112 10513  15K Jul 21  2020 test.tsv\n",
      "-rw-r--r-- 1 702112 10513 1.2M Jul 21  2020 train.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  sentence\n",
       "0  Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . . \n",
       "1  Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . .                                                                                                                                                                                                                                                                                                                                                                                                           "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_DIR + 'test.tsv', sep='\\t')\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a label column filled with the '0' value, which will be ignored during testing.\n",
    "test_df['label'] = 0\n",
    "test_df = test_df[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should see the headings we need: \"sentence\" and \"label\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma. In most patients with isolated unilateral retinoblastoma , tumor development is initiated by somatic inactivation of both alleles of the RB1 gene . However , some of these patients can transmit retinoblastoma predisposition to their offspring . To determine the frequency and nature of constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma , we analyzed DNA from peripheral blood and from tumor tissue . The analysis of tumors from 54 ( 71 % ) of 76 informative patients showed loss of constitutional heterozygosity ( LOH ) at intragenic loci . Three of 13 uninformative patients had constitutional deletions . For 39 randomly selected tumors , SSCP , hetero-duplex analysis , sequencing , and Southern blot analysis were used to identify mutations . Mutations were detected in 21 ( 91 % ) of 23 tumors with LOH . In 6 ( 38 % ) of 16 tumors without LOH , one mutation was detected , and in 9 ( 56 % ) of the tumors without LOH , both mutations were found . Thus , a total of 45 mutations were identified in tumors of 36 patients . Thirty-nine of the mutations-including 34 small mutations , 2 large structural alterations , and hypermethylation in 3 tumors-were not detected in the corresponding peripheral blood DNA . In 6 ( 17 % ) of the 36 patients , a mutation was detected in constitutional DNA , and 1 of these mutations is known to be associated with reduced expressivity . The presence of a constitutional mutation was not associated with an early age at treatment . In 1 patient , somatic mosaicism was demonstrated by molecular analysis of DNA and RNA from peripheral blood . In 2 patients without a detectable mutation in peripheral blood , mosaicism was suggested because 1 of the patients showed multifocal tumors and the other later developed bilateral retinoblastoma . In conclusion , our results emphasize that the manifestation and transmissibility of retinoblastoma depend on the nature of the first mutation , its time in development , and the number and types of cells that are affected . .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hereditary deficiency of the fifth component of complement in man. I. Clinical, immunochemical, and family studies. The first recognized human kindred with hereditary deficiency of the fifth component of complement ( C5 ) is described . The proband , a 20-year-old black female with systemic lupus erythematosus since age 11 , lacked serum hemolytic complement activity , even during remission . C5 was undetectable in her serum by both immunodiffusion and hemolytic assays . Other complement components were normal during remission of lupus , but C1 , C4 , C2 , and C3 levels fell during exacerbations . A younger half-sister , who had no underlying disease , was also found to lack immunochemically detectable C5 . By hemolytic assay , she exhibited 1-2 % of the normal serum C5 level and normal concentrations of other complement components . C5 levels of other family members were either normal or approximately half-normal , consistent with autosomal codominant inheritance of the gene determining C5 deficiency . Normal hemolytic titers were restored to both homozygous C5-deficient ( C5D ) sera by addition of highly purified human C5 . In specific C5 titrations , however , it was noted that when limited amounts of C5 were assayed in the presence of low dilutions of either C5D serum , curving rather than linear dose-response plots were consistently obtained , suggesting some inhibitory effect . Further studies suggested that low dilutions of C5D serum contain a factor ( or factors ) interfering at some step in the hemolytic assay of C5 , rather than a true C5 inhibitor or inactivator . Of clinical interest are ( a ) the documentation of membranous glomerulonephritis , vasculitis , and arthritis in an individual lacking C5 ( and its biologic functions ) , and ( b ) a remarkable propensity to bacterial infections in the proband , even during periods of low-dose or alternate-day corticosteroid therapy . Other observations indicate that the C5D state is compatible with normal coagulation function and the capacity to mount a neutrophilic leukocytosis during pyogenic infection . .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Susceptibility to ankylosing spondylitis in twins: the role of genes, HLA, and the environment. OBJECTIVE  To determine the relative effects of genetic and environmental factors in susceptibility to ankylosing spondylitis ( AS ) . METHODS  Twins with AS were identified from the Royal National Hospital for Rheumatic Diseases database . Clinical and radiographic examinations were performed to establish diagnoses , and disease severity was assessed using a combination of validated scoring systems . HLA typing for HLA-B27 , HLA-B60 , and HLA-DR1 was performed by polymerase chain reaction with sequence-specific primers , and zygosity was assessed using microsatellite markers . Genetic and environmental variance components were assessed with the program Mx , using data from this and previous studies of twins with AS . RESULTS  Six of 8 monozygotic ( MZ ) twin pairs were disease concordant , compared with 4 of 15 B27-positive dizygotic ( DZ ) twin pairs ( 27 % ) and 4 of 32 DZ twin pairs overall ( 12 . 5 % ) . Nonsignificant increases in similarity with regard to age at disease onset and all of the disease severity scores assessed were noted in disease-concordant MZ twins compared with concordant DZ twins . HLA-B27 and B60 were associated with the disease in probands , and the rate of disease concordance was significantly increased among DZ twin pairs in which the co-twin was positive for both B27 and DR1 . Additive genetic effects were estimated to contribute 97 % of the population variance . CONCLUSION  Susceptibility to AS is largely genetically determined , and the environmental trigger for the disease is probably ubiquitous . HLA-B27 accounts for a minority of the overall genetic susceptibility to AS .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             sentence  \\\n",
       "0  Clustering of missense mutations in the ataxia-telangiectasia gene in a sporadic T-cell leukaemia. Ataxia-telangiectasia ( A-T ) is a recessive multi-system disorder caused by mutations in the ATM gene at 11q22-q23 ( ref . 3 ) . The risk of cancer , especially lymphoid neoplasias , is substantially elevated in A-T patients and has long been associated with chromosomal instability . By analysing tumour DNA from patients with sporadic T-cell prolymphocytic leukaemia ( T-PLL ) , a rare clonal malignancy with similarities to a mature T-cell leukaemia seen in A-T , we demonstrate a high frequency of ATM mutations in T-PLL . In marked contrast to the ATM mutation pattern in A-T , the most frequent nucleotide changes in this leukaemia were missense mutations . These clustered in the region corresponding to the kinase domain , which is highly conserved in ATM-related proteins in mouse , yeast and Drosophila . The resulting amino-acid substitutions are predicted to interfere with ATP binding or substrate recognition . Two of seventeen mutated T-PLL samples had a previously reported A-T allele . In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia . Occasional missense mutations in ATM were also found in tumour DNA from patients with B-cell non-Hodgkins lymphomas ( B-NHL ) and a B-NHL cell line . The evidence of a significant proportion of loss-of-function mutations and a complete absence of the normal copy of ATM in the majority of mutated tumours establishes somatic inactivation of this gene in the pathogenesis of sporadic T-PLL and suggests that ATM acts as a tumour suppressor . As constitutional DNA was not available , a putative hereditary predisposition to T-PLL will require further investigation . .                                                                                                                                                                                                                                                                                                                               \n",
       "1  Myotonic dystrophy protein kinase is involved in the modulation of the Ca2+ homeostasis in skeletal muscle cells. Myotonic dystrophy ( DM ) , the most prevalent muscular disorder in adults , is caused by ( CTG ) n-repeat expansion in a gene encoding a protein kinase ( DM protein kinase ; DMPK ) and involves changes in cytoarchitecture and ion homeostasis . To obtain clues to the normal biological role of DMPK in cellular ion homeostasis , we have compared the resting [ Ca2 + ] i , the amplitude and shape of depolarization-induced Ca2 + transients , and the content of ATP-driven ion pumps in cultured skeletal muscle cells of wild-type and DMPK [ - / - ] knockout mice . In vitro-differentiated DMPK [ - / - ] myotubes exhibit a higher resting [ Ca2 + ] i than do wild-type myotubes because of an altered open probability of voltage-dependent l-type Ca2 + and Na + channels . The mutant myotubes exhibit smaller and slower Ca2 + responses upon triggering by acetylcholine or high external K + . In addition , we observed that these Ca2 + transients partially result from an influx of extracellular Ca2 + through the l-type Ca2 + channel . Neither the content nor the activity of Na + / K + ATPase and sarcoplasmic reticulum Ca2 + -ATPase are affected by DMPK absence . In conclusion , our data suggest that DMPK is involved in modulating the initial events of excitation-contraction coupling in skeletal muscle . .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "2  Constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma. In most patients with isolated unilateral retinoblastoma , tumor development is initiated by somatic inactivation of both alleles of the RB1 gene . However , some of these patients can transmit retinoblastoma predisposition to their offspring . To determine the frequency and nature of constitutional RB1-gene mutations in patients with isolated unilateral retinoblastoma , we analyzed DNA from peripheral blood and from tumor tissue . The analysis of tumors from 54 ( 71 % ) of 76 informative patients showed loss of constitutional heterozygosity ( LOH ) at intragenic loci . Three of 13 uninformative patients had constitutional deletions . For 39 randomly selected tumors , SSCP , hetero-duplex analysis , sequencing , and Southern blot analysis were used to identify mutations . Mutations were detected in 21 ( 91 % ) of 23 tumors with LOH . In 6 ( 38 % ) of 16 tumors without LOH , one mutation was detected , and in 9 ( 56 % ) of the tumors without LOH , both mutations were found . Thus , a total of 45 mutations were identified in tumors of 36 patients . Thirty-nine of the mutations-including 34 small mutations , 2 large structural alterations , and hypermethylation in 3 tumors-were not detected in the corresponding peripheral blood DNA . In 6 ( 17 % ) of the 36 patients , a mutation was detected in constitutional DNA , and 1 of these mutations is known to be associated with reduced expressivity . The presence of a constitutional mutation was not associated with an early age at treatment . In 1 patient , somatic mosaicism was demonstrated by molecular analysis of DNA and RNA from peripheral blood . In 2 patients without a detectable mutation in peripheral blood , mosaicism was suggested because 1 of the patients showed multifocal tumors and the other later developed bilateral retinoblastoma . In conclusion , our results emphasize that the manifestation and transmissibility of retinoblastoma depend on the nature of the first mutation , its time in development , and the number and types of cells that are affected . .    \n",
       "3  Hereditary deficiency of the fifth component of complement in man. I. Clinical, immunochemical, and family studies. The first recognized human kindred with hereditary deficiency of the fifth component of complement ( C5 ) is described . The proband , a 20-year-old black female with systemic lupus erythematosus since age 11 , lacked serum hemolytic complement activity , even during remission . C5 was undetectable in her serum by both immunodiffusion and hemolytic assays . Other complement components were normal during remission of lupus , but C1 , C4 , C2 , and C3 levels fell during exacerbations . A younger half-sister , who had no underlying disease , was also found to lack immunochemically detectable C5 . By hemolytic assay , she exhibited 1-2 % of the normal serum C5 level and normal concentrations of other complement components . C5 levels of other family members were either normal or approximately half-normal , consistent with autosomal codominant inheritance of the gene determining C5 deficiency . Normal hemolytic titers were restored to both homozygous C5-deficient ( C5D ) sera by addition of highly purified human C5 . In specific C5 titrations , however , it was noted that when limited amounts of C5 were assayed in the presence of low dilutions of either C5D serum , curving rather than linear dose-response plots were consistently obtained , suggesting some inhibitory effect . Further studies suggested that low dilutions of C5D serum contain a factor ( or factors ) interfering at some step in the hemolytic assay of C5 , rather than a true C5 inhibitor or inactivator . Of clinical interest are ( a ) the documentation of membranous glomerulonephritis , vasculitis , and arthritis in an individual lacking C5 ( and its biologic functions ) , and ( b ) a remarkable propensity to bacterial infections in the proband , even during periods of low-dose or alternate-day corticosteroid therapy . Other observations indicate that the C5D state is compatible with normal coagulation function and the capacity to mount a neutrophilic leukocytosis during pyogenic infection . .                                 \n",
       "4  Susceptibility to ankylosing spondylitis in twins: the role of genes, HLA, and the environment. OBJECTIVE  To determine the relative effects of genetic and environmental factors in susceptibility to ankylosing spondylitis ( AS ) . METHODS  Twins with AS were identified from the Royal National Hospital for Rheumatic Diseases database . Clinical and radiographic examinations were performed to establish diagnoses , and disease severity was assessed using a combination of validated scoring systems . HLA typing for HLA-B27 , HLA-B60 , and HLA-DR1 was performed by polymerase chain reaction with sequence-specific primers , and zygosity was assessed using microsatellite markers . Genetic and environmental variance components were assessed with the program Mx , using data from this and previous studies of twins with AS . RESULTS  Six of 8 monozygotic ( MZ ) twin pairs were disease concordant , compared with 4 of 15 B27-positive dizygotic ( DZ ) twin pairs ( 27 % ) and 4 of 32 DZ twin pairs overall ( 12 . 5 % ) . Nonsignificant increases in similarity with regard to age at disease onset and all of the disease severity scores assessed were noted in disease-concordant MZ twins compared with concordant DZ twins . HLA-B27 and B60 were associated with the disease in probands , and the rate of disease concordance was significantly increased among DZ twin pairs in which the co-twin was positive for both B27 and DR1 . Additive genetic effects were estimated to contribute 97 % of the population variance . CONCLUSION  Susceptibility to AS is largely genetically determined , and the environmental trigger for the disease is probably ubiquitous . HLA-B27 accounts for a minority of the overall genetic susceptibility to AS .                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "   label  \n",
       "0  0      \n",
       "1  0      \n",
       "2  0      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.4M\n",
      "-rw-r--r-- 1 702112 10513 133K Jul 21  2020 dev.tsv\n",
      "-rw-r--r-- 1 root   root   16K Jun  8 16:00 labeled_test.tsv\n",
      "-rw-r--r-- 1 702112 10513  15K Jul 21  2020 test.tsv\n",
      "-rw-r--r-- 1 702112 10513 1.2M Jul 21  2020 train.tsv\n"
     ]
    }
   ],
   "source": [
    "# Save the updated test file to disk\n",
    "test_df.to_csv(os.path.join(DATA_DIR, 'labeled_test.tsv'), sep='\\t', index=False)\n",
    "!ls -lh $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Exercise: Create Neural Modules\n",
    "You have everything you need to do most of this part yourself.  You're going to create the necessary neural modules for three pipelines:  training, validation, and test.  We always must begin by instantiating the `NeuralModuleFactory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural module factory\n",
    "nf = nemo.core.NeuralModuleFactory(log_dir=WORK_DIR,\n",
    "                                   create_tb_writer=True,\n",
    "                                   add_time_to_log_dir=False,\n",
    "                                   optimization_level=AMP_OPTIMIZATION_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, validation, and test each need a data layer neural module instantiated with `BertTextClassificationDataLayer`.  They will reuse the same language and classifier neural modules.  Only the training and validation pipelines will need a loss function neural module, which will be reused as well.\n",
    "\n",
    "In the next cell, the tokenizer and training data layer neural module are instantiated to get you started.  The tokenizer is required for the data layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-06-08 16:01:06 bert_tokenizer:78] Deriving bert model type from pretrained model name.\n",
      "[NeMo I 2021-06-08 16:01:06 text_classification_dataset:82] /dli/task/data/NCBI_tc-3/train.tsv: 962\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the data Layer neural module for training.\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle and use_cache to True for training  \n",
    "USE_CACHE = True\n",
    "tokenizer = get_tokenizer(tokenizer_name='nemobert', pretrained_model_name=PRETRAINED_MODEL_NAME)\n",
    "dl_train = BertTextClassificationDataLayer(input_file=os.path.join(DATA_DIR, 'train.tsv'),\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             max_seq_length=MAX_SEQ_LEN,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             use_cache=USE_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next five cells, instantiate:\n",
    "1. Validation data layer neural module\n",
    "1. Test data layer neural module\n",
    "1. Language model neural module\n",
    "1. Text classification model neural module\n",
    "1. Loss neural module\n",
    "\n",
    "Look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines.  If you get stuck, look back at the [2.0 NLP Projects with NeMo](020_ExploreNeMo.ipynb) notebook for inspiration or the [solution notebook](solution_notebooks/SOLN_030_TextClassification.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate the data Layer neural module for validation.\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle to False (the default value) and use_cache to True for validation  \n",
    "dl_val = None #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Instantiate the data Layer neural module for testing (inference).\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle to False (the default value) and use_cache to False (the default value) for testing  \n",
    "dl_test = None #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Instantiate the Language Model with the get_pretrained_lm_model function\n",
    "#    Include the pretrained_model_name as the parameter\n",
    "lm = None #FIXME\n",
    "\n",
    "# Sanity check the number of weight parameters\n",
    "#    It should be around 110M for `bert-base-uncased`\n",
    "print(f'{PRETRAINED_MODEL_NAME} has {lm.num_weights} weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Instantiate the SequenceClassifier\n",
    "#    Include the hidden_size, num_classes (which is 3), num_layers (set to 2), \n",
    "#    and a dropout rate of 0.1\n",
    "lm_hidden_size = lm.hidden_size\n",
    "mlp = None #FIXME\n",
    "\n",
    "# Compared to the language model, the MLP should be tiny (only 1/2 million).\n",
    "print(f'MLP has {mlp.num_weights} weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CrossEntropyLossNM Loss Function - no parameters required\n",
    "loss = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice job!  Your neural modules are set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Create Neural Graphs\n",
    "Define the neural graphs by linking the output of each neural module with the input of the next one in the pipeline.  If any of these fail, it may be because the neural module was not correctly instantiated in the previous exercise. These graphs define how data will flow through the neural modules. \n",
    "\n",
    "You will set up graphs for training and validation.  You'll set up the graph for testing (inference) after the network is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Training Graph\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/train_graph.png\" width=800>\n",
    "    <figcaption style=\"text-align:center;\">Training Graph</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the outputs for each neural module to define the inputs of the next one in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "train_input, train_token_types, train_attn_mask, train_labels = dl_train()\n",
    "\n",
    "# Define train_embeddings from the language model, based on inputs from the data layer  \n",
    "train_embeddings = lm(input_ids=train_input,\n",
    "                      token_type_ids=train_token_types,\n",
    "                      attention_mask=train_attn_mask)\n",
    "\n",
    "# Define the train_logits from the clasifier, based on the embeddings from the language model\n",
    "train_logits = mlp(hidden_states=train_embeddings)\n",
    "\n",
    "# Define the train_loss based on the classifier logits and data layer labels\n",
    "train_loss = loss(logits=train_logits, labels=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Exercise: Create the Validation Graph\n",
    "The validation graph is very similar to the training graph and uses most of the same neural modules. Only the data layer module is different. \n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/val_graph.png\" width=800>\n",
    "    <figcaption style=\"text-align:center;\">Validation Graph</figcaption>\n",
    "</figure>\n",
    "\n",
    "In the cell below, look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines (there are four of them).  If you get stuck, look back at the training graph you just set up for inspiration or the [solution notebook](solution_notebooks/SOLN_030_TextClassification.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "val_input, val_token_types, val_attn_mask, val_labels = None #FIXME\n",
    "\n",
    "# Define val_embeddings from the language model, based on inputs from the data layer  \n",
    "val_embeddings = None #FIXME\n",
    "\n",
    "# Define val_logits from the clasifier, based on the embeddings from the language model\n",
    "val_logits = None #FIXME\n",
    "\n",
    "# Define val_loss based on the classifier logits and data layer labels\n",
    "val_loss = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent!  You've set up the pipelines for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 Visualize Embeddings Before Training\n",
    "Before running the trainer, lets get visual idea of how well the language model you've chosen distinguishes between classes without any training.  As a quick proxy of how well the language model recognizes content within each class, we'll choose just words that are obviously representative of each of the three types of abstracts, which we'll call our \"spectrum words\".  \n",
    "* cancer category (0): 'cancer', 'lymphoma', 'melanoma', 'breast cancer', 'carcinoma'\n",
    "* neurological disorders (1): 'parkinson disease', 'neuropathy', 'muscular dystrophy', 'dystonia', 'hodgkin disease'\n",
    "* other (2): 'bacterial infection', 'genetic disease', 'eczema', 'diabetes', 'deficiency'\n",
    "\n",
    "In this visualization, we want to see how related the category words are to each other within the language model (not the classifier), so we'll examine the embedding tensors that come out of the language model in our graph.  Since language model embeddings are 768 dimensional for BERT base and 1024 dimensional for BERT large, we can't really visualize them directly, so we'll first apply [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1) and reduce the embeddings to only two dimensions, which we can plot on a two-dimensional graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spectrum words and store them just like a test set\n",
    "spectrum_words = ['cancer', 'lymphoma', 'melanoma', 'breast cancer', 'carcinoma',\n",
    "                  'parkinson disease', 'neuropathy', 'muscular dystrophy', 'dystonia', 'hodgkin disease',\n",
    "                  'bacterial infection', 'genetic disease', 'eczema', 'diabetes', 'deficiency',]\n",
    "\n",
    "spectrum_file = os.path.join(DATA_DIR, 'cancer_neuro_other.tsv')\n",
    "with open(spectrum_file, 'w+') as f:\n",
    "    f.write('sentence\\tlabel')\n",
    "    for word in spectrum_words:\n",
    "        f.write('\\n' + word + '\\t0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the spectrum set and check to see if it is in the right format\n",
    "spectrum_df = pd.read_csv(spectrum_file, delimiter='\\t')\n",
    "print(spectrum_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a data layer just for this mini-dataset of words, and a graph to match.  We won't need the classifier or loss neural modules for this graph, because we are not training or even classifying on the words.  We are just going to see the results of the words run through the language model, to see if they group together naturally by category.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/spectrum_graph.png\" width=400>\n",
    "    <figcaption style=\"text-align:center;\">Spectrum Graph</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a data layer neural module for the spectrum words\n",
    "dl_spectrum = BertTextClassificationDataLayer(input_file=spectrum_file,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                max_seq_length=MAX_SEQ_LEN,\n",
    "                                                batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the graph connections for the spectrum inference graph\n",
    "# There are only two neural modules in this graph: the data layer (dl_spectrum) and the language model already defined (lm)\n",
    "spectrum_input, spectrum_token_types, spectrum_attn_mask, spectrum_labels = dl_spectrum()\n",
    "spectrum_embeddings = lm(input_ids=spectrum_input,\n",
    "                           token_type_ids=spectrum_token_types,\n",
    "                           attention_mask=spectrum_attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the inference action (`nf.infer()`) through the spectrum graph, and sanity check the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings_tensors[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will create a figure to display where the words fall along the two t-SNE feature vectors.  Are the grouped together nicely? We'll check again after training to see if anything has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:5, 0], tsne_spectrum[0:5, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[5:10, 0], tsne_spectrum[5:10, 1], 'bo')\n",
    "plt.plot(tsne_spectrum[10:, 0], tsne_spectrum[10:, 1], 'g+')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Training\n",
    "Now that the graphs are set up, the action can begin.  You'll train the model with the NeuralModuleFactory `.train()` function. We need to set how many epochs and GPUs we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "NUM_GPUS = 1\n",
    "\n",
    "train_data_size = len(dl_train)\n",
    "steps_per_epoch = math.ceil(train_data_size / (BATCH_SIZE * NUM_GPUS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we also need to set the learning rate, optimizer, and callbacks for logging.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Set the Learning Rate and Optimizer\n",
    "For this project, we'll set the learning rate to 0.00005 and use a learning rate function, `WarmupAnnealing`.  We'll also use the popular [`adam`](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) optimization algorithm.  These values can be changed later if you wish.  We can see a list of available learning rate policies with the `get_all_lr_classes()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo.utils.lr_policies.get_all_lr_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'adam'\n",
    "LEARNING_RATE = 5e-5\n",
    "lr_policy_fn = get_lr_policy('WarmupAnnealing',\n",
    "                             total_steps=NUM_EPOCHS * steps_per_epoch,\n",
    "                             warmup_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Create the Callbacks\n",
    "Callbacks are used to record and log metrics and save checkpoints for the training and evaluation. We use callbacks to print to screen and also to tensorboard.  Note that the eval_callback is where the validation set is tested against the trained network for each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback for simple logging\n",
    "train_callback = SimpleLogger(step_freq=steps_per_epoch)\n",
    "\n",
    "# Create callback for tensorboard logging\n",
    "tensorboard_callback = TensorboardLogger(nf.tb_writer,\n",
    "                                        step_freq=steps_per_epoch)\n",
    "\n",
    "# Create callback to evaluate each epoch against the validation data\n",
    "eval_callback = EvaluatorCallback(eval_tensors=[val_logits, val_labels],\n",
    "                                            user_iter_callback=lambda x, y: eval_iter_callback(x, y, dl_val),\n",
    "                                            user_epochs_done_callback=lambda x:\n",
    "                                                eval_epochs_done_callback(x, f'{nf.work_dir}graphs'),\n",
    "                                            tb_writer=nf.tb_writer,\n",
    "                                            eval_step=steps_per_epoch)\n",
    "\n",
    "# Create callback to save checkpoints\n",
    "ckpt_callback = CheckpointCallback(folder=nf.checkpoint_dir,\n",
    "                                             epoch_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3 Run the Trainer\n",
    "It is not necessary to reset the trainer the first time, but the reset function is included here in case you want to change the number of epochs for more training. Reset is required in order to start the trainer again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo.core.NeuralModuleFactory.reset_trainer(nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nf.train(tensors_to_optimize=[train_loss],\n",
    "         callbacks=[train_callback, tensorboard_callback, eval_callback, ckpt_callback],\n",
    "         lr_policy=lr_policy_fn,\n",
    "         optimizer=OPTIMIZER,\n",
    "         optimization_params={'num_epochs': NUM_EPOCHS, 'lr': LEARNING_RATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.4 Visualize Embeddings After Training\n",
    "Update the embeddings with the newly trained language model and run the inference again.  Then, show the t-SNE plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_embeddings = lm(input_ids=spectrum_input,\n",
    "                           token_type_ids=spectrum_token_types,\n",
    "                           attention_mask=spectrum_attn_mask)\n",
    "spectrum_embeddings_tensors = nf.infer(tensors=[spectrum_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_activations = spectrum_embeddings_tensors[0][0][:,0,:].numpy()\n",
    "tsne_spectrum = TSNE(n_components=2, perplexity=10, verbose=1, learning_rate=2,\n",
    "                     random_state=123).fit_transform(spectrum_activations)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(tsne_spectrum[0:5, 0], tsne_spectrum[0:5, 1], 'rx')\n",
    "plt.plot(tsne_spectrum[5:10, 0], tsne_spectrum[5:10, 1], 'bo')\n",
    "plt.plot(tsne_spectrum[10:, 0], tsne_spectrum[10:, 1], 'g+')\n",
    "for (x,y, label) in zip(tsne_spectrum[0:, 0], tsne_spectrum[0:, 1], spectrum_df.sentence.values.tolist() ):\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Inference\n",
    "Now that the language model and classifier are trained with new weights, you can set up the training graph. Then, you can run inference on the test data and look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.1 Exercise: Create the Test Graph\n",
    "The test graph is a little different than the training and validation graphs.  For inference, we don't have any labels to learn from, and there is no loss function.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/test_graph.png\" width=600>\n",
    "    <figcaption style=\"text-align:center;\">Test Graph</figcaption>\n",
    "</figure>\n",
    "\n",
    "In the cell below, look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines (there are three of them).  If you get stuck, look back at the training graph you just set up for inspiration or the [solution notebook](solution_notebooks/SOLN_030_TextClassification.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "test_input, test_token_types, test_attn_mask, _ = None #FIXME\n",
    "\n",
    "# Define test_embeddings from the language model, based on inputs from the data layer  \n",
    "test_embeddings = None #FIXME\n",
    "\n",
    "# Define test_logits from the clasifier, based on the embeddings from the language model\n",
    "test_logits = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 Run Inference on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_logits_tensors = nf.infer(tensors=[test_logits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the probabilities produced for each of the three labels on each test abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = [torch.nn.functional.softmax(torch.cat(test_logits_tensors[0]), dim=1).numpy()[:, label_index] for label_index in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'labeled_test.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['prob'] = test_probs \n",
    "test_df['prob0'] = test_probs[0] \n",
    "test_df['prob1'] = test_probs[1]\n",
    "test_df['prob2'] = test_probs[2]\n",
    "inference_file = os.path.join(DATA_DIR, 'test_inference.tsv')\n",
    "test_df.to_csv(inference_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll create a function to use in a loop to output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classification(sample):\n",
    "    sentence = sample.sentence\n",
    "    prob0 = sample.prob0.values[0]\n",
    "    prob1 = sample.prob1.values[0]\n",
    "    prob2 = sample.prob2.values[0]\n",
    "    result = f'{sentence} \\n cancer | neurological | other\\n {prob0} | {prob1} | {prob2}'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.3 Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(inference_file, sep='\\t')\n",
    "for ind in range(len(df)):\n",
    "    sample = df[ind:ind+1]\n",
    "    print(sample_classification(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical results should looks something like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Risk reversals in predictive testing for Huntington disease. The first predictive testing for Huntington disease ( HD ) was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD . Limits to accuracy included recombination between the DNA markers and the mutation , pedigree structure , and whether DNA samples were available from family members . With direct tests for the HD mutation , we have assessed the accuracy of results obtained by linkage approaches when requested to do so by the test individuals . For six such individuals , there was significant disparity between the tests . Three went from a decreased risk to an increased risk , while in another three the risk was decreased . Knowledge of the potential reasons for these changes in results and impact of these risk reversals on both patients and the counseling team can assist in the development of strategies for the prevention and , where necessary , management of a risk reversal in any predictive testing program . . \n",
    "Name: sentence, dtype: object \n",
    " cancer | neurological | other\n",
    " 0.020393232 | 0.88683945 | 0.092767425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.4 Single Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(nf, tokenizer, bert, mlp, sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tmp_file = \"/tmp/tmp_sentence.tsv\"\n",
    "    with open(tmp_file, 'w+') as tmp_tsv:\n",
    "        header = 'sentence\\tlabel\\n'\n",
    "        line = sentence + '\\t0\\n'\n",
    "        tmp_tsv.writelines([header, line])\n",
    "\n",
    "    tmp_data = BertTextClassificationDataLayer(input_file=tmp_file,\n",
    "                                               tokenizer=tokenizer,\n",
    "                                               max_seq_length=128,\n",
    "                                               batch_size=1)\n",
    "    \n",
    "    tmp_input, tmp_token_types, tmp_attn_mask, _ = tmp_data()\n",
    "    tmp_embeddings = bert(input_ids=tmp_input,\n",
    "                          token_type_ids=tmp_token_types,\n",
    "                          attention_mask=tmp_attn_mask)\n",
    "    tmp_logits = mlp(hidden_states=tmp_embeddings)\n",
    "    tmp_logits_tensors = nf.infer(tensors=[tmp_logits, tmp_embeddings])\n",
    "    tmp_probs0 = torch.nn.functional.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 0]\n",
    "    tmp_probs1 = torch.nn.functional.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 1]\n",
    "    tmp_probs2 = torch.nn.functional.softmax(torch.cat(tmp_logits_tensors[0])).numpy()[:, 2]\n",
    "    print(f'\\n******\\n{sentence} \\n {tmp_probs0[0]} | {tmp_probs1[0]} | {tmp_probs2[0]}\\n********\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = ['In contrast , no mutations were detected in the p53 gene , suggesting that this tumour suppressor is not frequently altered in this leukaemia .',\n",
    "             'The first predictive testing for Huntington disease ( HD ) was based on analysis of linked polymorphic DNA markers to estimate the likelihood of inheriting the mutation for HD .',\n",
    "             'Germ-line mutations of the BRCA1 gene predispose women to early-onset breast and ovarian cancer by compromising the genes presumptive function as a tumor suppressor .',\n",
    "             'Further studies suggested that low dilutions of C5D serum contain a factor ( or factors ) interfering at some step in the hemolytic assay of C5 , rather than a true C5 inhibitor or inactivator .'\n",
    "            ]\n",
    "# should be 0, 1, 0, 2 \n",
    "for sentence in sentences:\n",
    "    classify_sentence(nf, tokenizer, lm, mlp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Exercise: Change the Language Model\n",
    "Now that you've built the project, you can experiment with different settings, especially different language models.  For this exercise, try 'bert-large-uncased' or 'megatron-uncased'.  To do that, you'll need to restart the kernel to clear memory, and change the parameters in the [3.1.1 Input Parameters](#3.1.1-Input-Parameters) section.  With these larger models, you'll need to reduce the batch size to avoid an out-of-memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've built a text classifier with three classes and learned:\n",
    "* How to fine-tune a BERT-based language model for text classification in NeMo\n",
    "* How to select different pre-trained language models\n",
    "* How to visualize model accuracy\n",
    "* How to test your model using inference\n",
    "\n",
    "You're ready to try a different NLP task.<br>\n",
    "\n",
    "Move on to [4.0 Build a Named Entity Recognizer](040_NamedEntityRecognition.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
