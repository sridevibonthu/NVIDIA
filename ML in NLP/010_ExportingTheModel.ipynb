{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Exporting the Model\n",
    "In this notebook, you'll explore options for exporting a BERT checkpoint trained using PyTorch, to NVIDIA Triton Inference Server.\n",
    "\n",
    "**[1.1 Overview: Optimization and Performance](#1.1-Overview:-Optimization-and-Performance)<br>**\n",
    "**[1.2 Export a BERT Checkpoint](#1.2-Export-a-BERT-Checkpoint)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.1 Triton Model Repository](#1.2.1-Triton-Model-Repository)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.2 TorchScript Export](#1.2.2-TorchScript-Export)<br>\n",
    "**[1.3 Test Our Export](#1.3-Test-Our-Export)<br>**\n",
    "**[1.4 Beyond TorchScript](#1.4-Beyond-TorchScript)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.4.1 Exercise: Enable TensorRT Optimization](#1.4.1-Exercise:-Enable-TensorRT-Optimization)<br>\n",
    "**[1.5 Performance Comparison](#1.5-Performance-Comparison)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Overview: Optimization and Performance\n",
    "Optimization of the trained model will have a fairly dramatic impact on the inference performance, measured in bandwidth and latency. Even if the project requirements do not justify investing engineering effort into advanced techniques, such as knowledge distillation or pruning, a fair amount of model performance improvement can be achieved by using model optimization tools. The diagram below illustrates the difference in inference performance between a model deployed using non-optimized TensorFlow, the same model post-processed with TensorRT, and a model fully optimized with TensorRT. \n",
    "\n",
    "<img src=\"images/TFvTRT.jpg\" alt=\"Header\" style=\"width: 600px;\"/>\n",
    "\n",
    "Modern inference servers typically support substantially more than one model format to cater to a wider range of projects, tools, and preferences. Since in this class we are working with a BERT checkpoint trained using PyTorch, and we are deploying it with Triton Inference Server, we will focus on options for deploying PyTorch-based models. These include:\n",
    "   - PyTorch JIT / TorchScript\n",
    "   - ONNX runtime\n",
    "   - ONNX-TensorRT\n",
    "   - TensorRT\n",
    "    \n",
    "It's important to point out that Triton Server supports a much broader set of deployment mechanisms including:\n",
    "   - TensorFlow GraphDef\n",
    "   - TensorFlow saved model\n",
    "   - Caffe 2 exports\n",
    "   - Custom models (which can be any custom executable)\n",
    "\n",
    "In this section we will look at how to deploy a model using some of the deployment engines listed above and the impact each has on performance. We will also experiment with some of the key settings, namely the batch size and numerical precision (FP32 and FP16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Export a BERT Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model checkpoint we want to deploy, <code>bert_qa.pt</code>, should be located in your `data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bert_qa.pt\n"
     ]
    }
   ],
   "source": [
    "!ls data/*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is a standard checkpoint of a BERT-Large network, fine-tuned on the [Stanford Question Answering Dataset (SQuAD)](https://arxiv.org/abs/1606.05250). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Scripts\n",
    "As we explore various deployment configurations, we'll repeat some steps over and over.  Therefore, we'll use some helper scripts to partially automate the process so that we can focus our attention on the configuration settings and results.  You can explore the code details yourself if you are curious:\n",
    "\n",
    "- [utilities/wait_for_triton_server.sh](utilities/wait_for_triton_server.sh): Check the \"live\" and \"ready\" status of the Triton server via the API\n",
    "- [deployer/deployer.py](deployer/deployer.py): Convert a checkpoint to a deployable model and export it\n",
    "- [uitlities/run_perf_client_local.sh](utilities/run_perf_client_local.sh): Measure performance with the [perf_client](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/perf_client.html) application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Triton server has been deployed in a container and is available to us at host \"triton\" on port \"8000\". Run the next cell to to check for a \"200 OK\" HTTP response from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Triton Model Repository\n",
    "When Triton Server is started, it is typically configured to observe a local or remote file system where models are hosted. The directory which is being observed is called a *model repository*. A typical command to start the Triton Server identifies the location of the model repository with an option:<br>\n",
    "```bash\n",
    "tritonserver --model-repository=\"/path/to/model/repository\"\n",
    "```\n",
    "\n",
    "The model repository needs to have the following layout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "<model-repository-path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab container is configured to use the <code>./model_repository</code> folder as the model repository, so any change within this folder will affect the behavior of Triton Server.<br/>\n",
    "\n",
    "In order to expose a new model to Triton you need to: <br/>\n",
    "   1. Create a new model folder in the model repository. The name of the folder needs to reflect the name of the service you will be exposing to your users/applications.<br/>\n",
    "   2. Within the model folder, create a <code>config.pbtxt</code> file that contains the basic serving configuration for the model<br/>\n",
    "   3. Also within the model folder, create at least one folder containing a copy of the model. The name of the folder reflects the version name of the model. You can create and host multiple versions of the same model.<br/>\n",
    "    \n",
    "Next, we'll walk through the process of exporting the model to Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 TorchScript Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab we will:\n",
    "   - Convert the PyTorch checkpoint into [TorchScript](https://pytorch.org/docs/stable/jit.html#torchscript)\n",
    "   - Generate the Triton configuration file\n",
    "   - Deploy the created assets to our model repository\n",
    "Please execute the cells below. Since we are loading a PyTorch checkpoint and converting it into TorchScript, it might take a minute or two to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-torchscript in format pytorch_libtorch\n",
      "/opt/conda/lib/python3.6/site-packages/torch/jit/_recursive.py:160: UserWarning: 'bias' was found in ScriptModule constants,  but it is a non-constant parameter. Consider removing it.\n",
      "  \" but it is a non-constant {}. Consider removing it.\".format(name, hint))\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  8.344650268554688e-06\n",
      "\n",
      "average L_inf error over output tensors:  8.106231689453125e-06\n",
      "variance of L_inf error over output tensors:  7.579122514774402e-14\n",
      "stddev of L_inf error over output tensors:  2.7530206164819037e-07\n",
      "\n",
      "time of error check of native model:  0.7169404029846191 seconds\n",
      "time of error check of ts model:  1.891817569732666 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --ts-script \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint \"/dli/task/data/bert_qa.pt\" \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deployer.py` script loads the `bert_qa.pt` checkpoint, deploys it in `ts-script` format into a folder called `bertQA-torchscript`, and marks it as version `1`. We will discuss some of the more advanced settings later. For now, let's inspect the files generated by the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Jun  9 12:35 .\n",
      "drwxr-xr-x 3 root root 4096 Jun  9 12:35 ..\n",
      "drwxr-xr-x 2 root root 4096 Jun  9 12:35 1\n",
      "-rw-r--r-- 1 root root  568 Jun  9 12:35 config.pbtxt\n",
      "total 1309292\n",
      "drwxr-xr-x 2 root root       4096 Jun  9 12:35 .\n",
      "drwxr-xr-x 3 root root       4096 Jun  9 12:35 ..\n",
      "-rw-r--r-- 1 root root 1340706048 Jun  9 12:35 model.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-torchscript/\n",
    "!ls -al ./candidatemodels/bertQA-torchscript/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the script exported the model into the TorchScript format and saved it as `model.pt`. It also generated the `config.pbtxt` file. <br> \n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"bertQA-torchscript\"\n",
      "platform: \"pytorch_libtorch\"\n",
      "max_batch_size: 8\n",
      "input [\n",
      "{\n",
      "    name: \"input__0\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__1\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__2\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "output [\n",
      "{\n",
      "    name: \"output__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}, \n",
      "{\n",
      "    name: \"output__1\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "optimization {\n",
      "  cuda {\n",
      "    graphs: 0\n",
      "  }\n",
      "}\n",
      "instance_group [\n",
      "    {\n",
      "        count: 1\n",
      "        kind: KIND_GPU\n",
      "        gpus: [ 0 ]\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat ./candidatemodels/bertQA-torchscript/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is fairly simple and defines:\n",
    "   - Name of the model\n",
    "   - Type of platform to be used for inference; in this case `pytorch_libtorch`\n",
    "   - Input and output dimensions used by the network\n",
    "   - Optimizations used; in this case GPU and the default TorchScript optimization \n",
    "   - Instance group configuration; in this case instance group count is set to one, meaning that only one copy of the model will be held in GPU memory (GPU 0 is being used).\n",
    "    \n",
    "To deploy the model, move the folder to the Triton model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-torchscript model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You have successfully deployed your first model to Triton Inference Server!\n",
    "\n",
    "We'll come back to discuss the detailed configuration later, but for now let's see how our model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.3 Test Our Export\n",
    "Execute the cells below to start an inference process and make a simple measurement of inference performance. First, we'll set up some configuration. `maxConcurrency` is set to two, meaning that the stress test will be executed twice. The first run will use just a single thread and the second one will use two threads to query the server. Without turning on the concurrent model execution or dynamic batching features, what do you think will be the impact on performance of running two processes querying the server? Do you think:<br/>\n",
    "- Bandwidth will increase or decrease?<br/>\n",
    "- Latency will increase or decrease?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 2 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 24 infer/sec. Avg latency: 45205 usec (std 85813 usec)\n",
      "  Pass [2] throughput: 29 infer/sec. Avg latency: 34708 usec (std 299 usec)\n",
      "  Pass [3] throughput: 29 infer/sec. Avg latency: 34685 usec (std 210 usec)\n",
      "  Pass [4] throughput: 29 infer/sec. Avg latency: 34707 usec (std 233 usec)\n",
      "  Client: \n",
      "    Request count: 87\n",
      "    Throughput: 29 infer/sec\n",
      "    Avg latency: 34707 usec (standard deviation 233 usec)\n",
      "    p50 latency: 34621 usec\n",
      "    p90 latency: 34998 usec\n",
      "    p95 latency: 35205 usec\n",
      "    p99 latency: 35556 usec\n",
      "    Avg HTTP time: 34694 usec (send 5 usec + response wait 34688 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 104\n",
      "    Execution count: 104\n",
      "    Successful request count: 104\n",
      "    Avg request latency: 34345 usec (overhead 2 usec + queue 772 usec + compute input 59 usec + compute infer 33265 usec + compute output 247 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 29 infer/sec. Avg latency: 69273 usec (std 252 usec)\n",
      "  Pass [2] throughput: 29 infer/sec. Avg latency: 69406 usec (std 414 usec)\n",
      "  Pass [3] throughput: 28.6667 infer/sec. Avg latency: 69376 usec (std 449 usec)\n",
      "  Client: \n",
      "    Request count: 86\n",
      "    Throughput: 28.6667 infer/sec\n",
      "    Avg latency: 69376 usec (standard deviation 449 usec)\n",
      "    p50 latency: 69244 usec\n",
      "    p90 latency: 69736 usec\n",
      "    p95 latency: 69865 usec\n",
      "    p99 latency: 71817 usec\n",
      "    Avg HTTP time: 69358 usec (send 5 usec + response wait 69352 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 104\n",
      "    Execution count: 104\n",
      "    Successful request count: 104\n",
      "    Avg request latency: 68994 usec (overhead 3 usec + queue 35428 usec + compute input 57 usec + compute infer 33263 usec + compute output 243 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 29 infer/sec, latency 34707 usec\n",
      "Concurrency: 2, throughput: 28.6667 infer/sec, latency 69376 usec\n"
     ]
    }
   ],
   "source": [
    "!./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went okay you should have been presented with output similar to the following example result, showing the inference performance across two different configurations.<br/>\n",
    "<img src=\"images/InferenceJob1.png\" alt=\"Example output of inference job 1\" style=\"width: 1200px;\"/>\n",
    "\n",
    "If you happened to get \"error: failed to get model metatdata\", try running the cell again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Beyond TorchScript\n",
    "\n",
    "Let's investigate a different route for model deployment onto Triton, namely <a href=\"https://onnx.ai\">Open Neural Network Exchange (ONNX)</a>. ONNX is an open format for representation and exchange of neural network models. It defines a common set of operators that are used to build common models, as well as a file format for exchanging them. The advantage of ONNX is that it is relatively widely adopted and can be used to exchange models between <a href=\"https://onnx.ai/supported-tools.html\">a wide range of deep learning tools</a>, such as deep learning frameworks or deployment tools. This also includes TensorRT, which can consume ONNX models. </br>\n",
    "\n",
    "As before, start by exporting the model, but this time using the ONNX format. We will take advantage of the export tool that we used earlier, but change the export format from <code>ts-script</code> to <code>onnx</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4401233196258545 seconds\n",
      "time of error check of onnx model:  21.594746828079224 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md\">TorchScript serialization format</a>, the <a href=\"https://onnx.ai/get-started.html\">ONNX format</a> can be inspected quite easily (and parts are human readable). Lets have a look at the assets our export has generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Jun  9 12:41 .\n",
      "drwxr-xr-x 3 root root 4096 Jun  9 12:40 ..\n",
      "drwxr-xr-x 2 root root 4096 Jun  9 12:41 1\n",
      "-rw-r--r-- 1 root root  561 Jun  9 12:41 config.pbtxt\n",
      "total 1305228\n",
      "drwxr-xr-x 2 root root       4096 Jun  9 12:41 .\n",
      "drwxr-xr-x 3 root root       4096 Jun  9 12:41 ..\n",
      "-rw-r--r-- 1 root root 1336539973 Jun  9 12:41 model.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx/\n",
    "!ls -al ./candidatemodels/bertQA-onnx/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have a configuration file as well as a model, this time stored in ONNX format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for executing the ONNX-based export in Triton:\n",
    "- We can take advantage of ONNX runtime </br>\n",
    "- We can ask TensorRT to parse the ONNX assets in order to generate a TensorRT engine to use instead </br>\n",
    "\n",
    "We'll try both approaches and look at the impact this has on inference performance. In order to deploy the current ONNX model, move it to the model repository..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run our stress testing code across 10 different levels of concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 164279 usec (std 1919 usec)\n",
      "  Pass [2] throughput: 50.6667 infer/sec. Avg latency: 164529 usec (std 1233 usec)\n",
      "  Pass [3] throughput: 50.6667 infer/sec. Avg latency: 164504 usec (std 813 usec)\n",
      "  Client: \n",
      "    Request count: 19\n",
      "    Throughput: 50.6667 infer/sec\n",
      "    Avg latency: 164504 usec (standard deviation 813 usec)\n",
      "    p50 latency: 164462 usec\n",
      "    p90 latency: 165362 usec\n",
      "    p95 latency: 165426 usec\n",
      "    p99 latency: 166583 usec\n",
      "    Avg HTTP time: 164422 usec (send 16 usec + response wait 164404 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 163946 usec (overhead 3 usec + queue 31 usec + compute input 25 usec + compute infer 163863 usec + compute output 24 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 50.6667 infer/sec. Avg latency: 327134 usec (std 20053 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 331201 usec (std 1285 usec)\n",
      "  Pass [3] throughput: 50.6667 infer/sec. Avg latency: 331709 usec (std 1284 usec)\n",
      "  Client: \n",
      "    Request count: 19\n",
      "    Throughput: 50.6667 infer/sec\n",
      "    Avg latency: 331709 usec (standard deviation 1284 usec)\n",
      "    p50 latency: 331379 usec\n",
      "    p90 latency: 333228 usec\n",
      "    p95 latency: 333525 usec\n",
      "    p99 latency: 335338 usec\n",
      "    Avg HTTP time: 331771 usec (send 23 usec + response wait 331746 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 168\n",
      "    Execution count: 21\n",
      "    Successful request count: 21\n",
      "    Avg request latency: 331213 usec (overhead 4 usec + queue 165371 usec + compute input 27 usec + compute infer 165787 usec + compute output 24 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 480777 usec (std 45264 usec)\n",
      "  Pass [2] throughput: 50.6667 infer/sec. Avg latency: 498063 usec (std 1316 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 496843 usec (std 882 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 496843 usec (standard deviation 882 usec)\n",
      "    p50 latency: 496674 usec\n",
      "    p90 latency: 498118 usec\n",
      "    p95 latency: 498206 usec\n",
      "    p99 latency: 498398 usec\n",
      "    Avg HTTP time: 496927 usec (send 20 usec + response wait 496906 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 168\n",
      "    Execution count: 21\n",
      "    Successful request count: 21\n",
      "    Avg request latency: 496362 usec (overhead 2 usec + queue 330816 usec + compute input 23 usec + compute infer 165501 usec + compute output 20 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 50.6667 infer/sec. Avg latency: 628965 usec (std 66132 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 664349 usec (std 1236 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 662804 usec (std 1270 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 662804 usec (standard deviation 1270 usec)\n",
      "    p50 latency: 662944 usec\n",
      "    p90 latency: 663736 usec\n",
      "    p95 latency: 664311 usec\n",
      "    p99 latency: 666207 usec\n",
      "    Avg HTTP time: 662757 usec (send 26 usec + response wait 662729 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 662180 usec (overhead 4 usec + queue 496556 usec + compute input 28 usec + compute infer 165572 usec + compute output 20 usec)\n",
      "\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 50.6667 infer/sec, latency 164504 usec\n",
      "Concurrency: 2, throughput: 50.6667 infer/sec, latency 331709 usec\n",
      "Concurrency: 3, throughput: 48 infer/sec, latency 496843 usec\n",
      "Concurrency: 4, throughput: 48 infer/sec, latency 662804 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the results. Did we manage to run our benchmark at all 10 concurrency levels (or did the benchmark time out earlier)? What happened to the request latency in relation to the 500 ms time limit we configured?</br>\n",
    "\n",
    "Now let's export the ONNX model again, so that we can configure it for TensorRT execution.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-fp16 in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.42687535285949707 seconds\n",
      "time of error check of onnx model:  17.998917818069458 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the above command should have generated the ONNX export as well as a configuration file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Jun  9 12:44 .\n",
      "drwxr-xr-x 3 root root 4096 Jun  9 12:44 ..\n",
      "drwxr-xr-x 2 root root 4096 Jun  9 12:44 1\n",
      "-rw-r--r-- 1 root root  570 Jun  9 12:44 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx-trt-fp16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 Exercise: Enable TensorRT Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enable TensorRT, we need to add an additional section to the \"config.pbtxt\" configuration file. In particular, we need to add an additional segment to the <code>optimization</code> section:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt) to enable TensorRT. Feel free to look at the [solution](solutions/ex-1-4-1_config.pbtxt) as needed.\n",
    "2. Once you have saved your changes (Main menu: File -> Save File), move the folder to the model repository using the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-fp16 model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Execute our profiling tool in the next cell and investigate the impact on performance. This could take a while to start, as we are waiting for the server to migrate the model to TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 163123 usec (std 1684 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 162550 usec (std 146 usec)\n",
      "  Pass [3] throughput: 50.6667 infer/sec. Avg latency: 163528 usec (std 519 usec)\n",
      "  Client: \n",
      "    Request count: 19\n",
      "    Throughput: 50.6667 infer/sec\n",
      "    Avg latency: 163528 usec (standard deviation 519 usec)\n",
      "    p50 latency: 163774 usec\n",
      "    p90 latency: 164103 usec\n",
      "    p95 latency: 164105 usec\n",
      "    p99 latency: 164187 usec\n",
      "    Avg HTTP time: 163544 usec (send 19 usec + response wait 163523 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 163008 usec (overhead 3 usec + queue 34 usec + compute input 26 usec + compute infer 162918 usec + compute output 27 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 50.6667 infer/sec. Avg latency: 326369 usec (std 3018 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 327937 usec (std 209 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 327659 usec (std 340 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 327659 usec (standard deviation 340 usec)\n",
      "    p50 latency: 327585 usec\n",
      "    p90 latency: 328019 usec\n",
      "    p95 latency: 328350 usec\n",
      "    p99 latency: 328433 usec\n",
      "    Avg HTTP time: 327641 usec (send 23 usec + response wait 327616 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 327014 usec (overhead 4 usec + queue 163348 usec + compute input 33 usec + compute infer 163601 usec + compute output 28 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 481874 usec (std 37516 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 491613 usec (std 494 usec)\n",
      "  Pass [3] throughput: 50.6667 infer/sec. Avg latency: 491815 usec (std 402 usec)\n",
      "  Client: \n",
      "    Request count: 19\n",
      "    Throughput: 50.6667 infer/sec\n",
      "    Avg latency: 491815 usec (standard deviation 402 usec)\n",
      "    p50 latency: 491743 usec\n",
      "    p90 latency: 492303 usec\n",
      "    p95 latency: 492396 usec\n",
      "    p99 latency: 492407 usec\n",
      "    Avg HTTP time: 491778 usec (send 27 usec + response wait 491748 usec + receive 3 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 491100 usec (overhead 3 usec + queue 327336 usec + compute input 39 usec + compute infer 163688 usec + compute output 34 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 50.6667 infer/sec. Avg latency: 621420 usec (std 66184 usec)\n",
      "  Pass [2] throughput: 48 infer/sec. Avg latency: 655396 usec (std 466 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 656674 usec (std 978 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 656674 usec (standard deviation 978 usec)\n",
      "    p50 latency: 656377 usec\n",
      "    p90 latency: 658376 usec\n",
      "    p95 latency: 658436 usec\n",
      "    p99 latency: 658461 usec\n",
      "    Avg HTTP time: 656522 usec (send 28 usec + response wait 656491 usec + receive 3 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 655813 usec (overhead 4 usec + queue 491857 usec + compute input 42 usec + compute infer 163871 usec + compute output 39 usec)\n",
      "\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 50.6667 infer/sec, latency 163528 usec\n",
      "Concurrency: 2, throughput: 48 infer/sec, latency 327659 usec\n",
      "Concurrency: 3, throughput: 50.6667 infer/sec, latency 491815 usec\n",
      "Concurrency: 4, throughput: 48 infer/sec, latency 656674 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"8\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the performance against ONNX runtime. \n",
    "* How did the latency change, especially across larger concurrency runs? \n",
    "* How did the bandwidth change? Can you explain the level of bandwidth change observed? \n",
    "* Why did the ONNX model timeout at concurrency of less than 10? How does the TensorRT latency at concurrency 10 compare to latency of pure ONNX runtime at an earlier concurrency?\n",
    "\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've successfully deployed an NLP model to Triton Server with TorchScript and applied both reduced precision and TensorRT optimizations.\n",
    "In the next notebook you'll learn how to optimize the model itself and to deploy it in an efficient way. \n",
    "\n",
    "Please proceed to the next notebook:<br>\n",
    "[2.0 Hosting the model](020_HostingTheModel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
