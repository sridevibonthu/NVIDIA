{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Build a Named Entity Recognizer\n",
    "\n",
    "In this notebook, you'll build an application that finds disease names in medical disease abstracts. The model does not \"search\" for names from a list, but rather \"recognizes\" that certain words are disease references from the context of the language.\n",
    "\n",
    "**[4.1 Set Up the Project](#4.1-Set-Up-the-Project)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.1.1 Input Parameters](#4.1.1-Input-Parameters)<br>\n",
    "**[4.2 Create Neural Modules](#4.2-Create-Neural-Modules)**<br>\n",
    "**[4.3 Create Neural Graphs](#4.3-Create-Neural-Graphs)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.3.1 Training Graph](#4.3.1-Training-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.3.2 Exercise: Create the Validation Graph](#4.3.2-Exercise:-Create-the-Validation-Graph)<br>\n",
    "**[4.4 Training](#4.4-Training)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.1 Set the Learning Rate and Optimizer](#4.4.1-Set-the-Learning-Rate-and-Optimizer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.2 Exercise: Create the Callbacks](#4.4.2-Exercise:-Create-the-Callbacks)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.3 Run the Trainer](#4.4.3-Run-the-Trainer)<br>\n",
    "**[4.5 Inference](#4.5-Inference)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.5.1 Create the Test Graph](#4.5.1-Create-the-Test-Graph)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.5.2 Run Inference on the Test Queries](#4.5.2-Run-Inference-on-the-Test-Queries)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.5.3 Inference Results](#4.5.3-Inference-Results)<br>\n",
    "**[4.6 Exercise: Change the Language Model](#4.6-Exercise:-Change-the-Language-Model)**<br>\n",
    "\n",
    "As we saw in the [1.0 Explore the Data](010_ExploreData.ipynb) notebook, the dataset for the NER project is made up of sentences with IOB tagging, where each word in a sentence is tagged as inside, outside, or the beginning of a named entity.  In the NER task, you'll follow the same basic steps as in the text classification task to build your project, train it, and test it, with a few differences:\n",
    "* You'll learn to apply the *domain-specific* [BioBERT](https://news.developer.nvidia.com/biobert-optimized/) language model from an imported checkpoint, instead of using one of the default pretrained models\n",
    "* You'll use different neural modules appropriate for use with NER\n",
    "* You'll learn to use a query-based technique for inference\n",
    "\n",
    "BioBERT has the same network architecture as the original BERT, but instead of Wikipedia and BookCorpus, it is pretrained on PubMed, a large biomedical text corpus.  Starting with BioBERT instead of BERT achieves better performance in biomedical downstream tasks, such as question answering(QA), named entity recognition(NER) and relationship extraction(RE). This model was trained for 1M steps. For more information please refer to the original paper: [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Set Up the Project\n",
    "The NeMo setup pattern is the same for NER as for text classification, but some of the details are different.\n",
    "* The \"input data\" consists of two files instead of one (text and labels)\n",
    "* The data layers and classifiers work with \"tokens\" \n",
    "* We need to define a \"none\" label for the token padding, which we'll designate as \"O\", the symbol for \"outside\" in IOB\n",
    "\n",
    "<img src=\"../images/nemo/nm-pipe.png\" width=800>\n",
    "\n",
    "Begin by importing libraries.  Note the specific classes and helpers that are imported below.  These are the ones you'll use to build your training, validation, and testing graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful math and utility libraries\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import errno\n",
    "import inspect\n",
    "import termcolor\n",
    "\n",
    "# Import the nemo toolkit and NLP libraries\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "\n",
    "# Import the specific neural modules and module helpers we need\n",
    "from nemo.collections.nlp.nm.data_layers import BertTokenClassificationDataLayer\n",
    "from nemo.collections.nlp.nm.data_layers import BertTokenClassificationInferDataLayer\n",
    "from nemo.collections.nlp.nm.trainables import get_pretrained_lm_model\n",
    "from nemo.collections.nlp.nm.trainables import TokenClassifier\n",
    "from nemo.backends.pytorch.common.losses import CrossEntropyLossNM\n",
    "\n",
    "# Import helpers for fetching learning rate policy, tokenizer, vocabulary\n",
    "from nemo.utils.lr_policies import get_lr_policy\n",
    "from nemo.collections.nlp.data.tokenizers import get_tokenizer\n",
    "from nemo.collections.nlp.utils.data_utils import get_vocab\n",
    "\n",
    "# Import callbacks and callback functions\n",
    "from nemo.core import SimpleLogger, TensorboardLogger, EvaluatorCallback, CheckpointCallback\n",
    "from nemo.collections.nlp.callbacks.token_classification_callback import eval_epochs_done_callback, eval_iter_callback\n",
    "from nemo import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Input Parameters\n",
    "The training text and label files are `text_train.txt` and `labels_train`, respectively.  The validation and test files follow a similar naming pattern. Verify the location of the data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /dli/task/data/NCBI_ner-3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOB Tagging\n",
    "Recall that the sentences and labels in the NER dataset map to each other with _inside, outside, beginning (IOB)_ tagging.\n",
    "This mechanism can be used in a general way for multiple named entity types:\n",
    "* B-{CHUNK_TYPE} – for the word in the Beginning chunk\n",
    "* I-{CHUNK_TYPE} – for words Inside the chunk\n",
    "* O – Outside any chunk\n",
    "\n",
    "In our case, we are only looking for \"disease\" as our entity (or chunk) type, so we don't need to identify beyond the three classes: I, O, and B.\n",
    "**Three classes**\n",
    "* B - Beginning of disease name\n",
    "* I - Inside word of disease name\n",
    "* O - Outside of all disease names\n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "If we were looking for two kinds of named entities, such as nouns and verbs in a parts-of-speech analysis, we would use a five-class IOB scheme:<br>\n",
    "**Five classes**\n",
    "* B-N - Beginning of noun word or phrase\n",
    "* I-N - Inside noun word or phrase\n",
    "* B-V - Beginning of verb word or phrase\n",
    "* I-V - Inside verb word or phrase\n",
    "* O   - Outside all nouns and verbs\n",
    "\n",
    "If you are intereested in learning more, take a look at [this paper](http://cs229.stanford.edu/proj2005/KrishnanGanapathy-NamedEntityRecognition.pdf) on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-Specific Checkpoint\n",
    "We also need the checkpoints and configuration files for our domain-specific language models.  Specific checkpoints pretrained for BioBERT and BioMegatron can be downloaded from [NVIDIA NGC Models](https://ngc.nvidia.com/catalog/models):<br><br>\n",
    "For BioBERT  https://ngc.nvidia.com/catalog/models/nvidia:biobertbasecasedfornemo.<br>\n",
    "For BioMegatron  https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345muncased.\n",
    "\n",
    "**You don't need to download them for this course**, as that has already been done.  Run the next cell to see where the checkpoint files (.pt) and config files (.json) are located in the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree /ngc_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up all the locations and pre-set parameters for the model.  If you want to try a different model later, you can restart the notebook kernel, then change MODEL_TYPE here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the input data location\n",
    "DATA_DIR = '/dli/task/data/NCBI_ner-3/'\n",
    "\n",
    "# Identify downloaded checkpoints location\n",
    "CHECKPOINTS_PATH = '/ngc_checkpoints/checkpoints/' \n",
    "\n",
    "# Select which model you want to use, either biobert or biomegatron\n",
    "MODEL_TYPE = \"biobert\"\n",
    "# MODEL_TYPE = \"biomegatron\"\n",
    "\n",
    "# Set the number of words in the sequences\n",
    "# Shorter sequences will be padded with NONE_LABEL, longer ones truncated\n",
    "MAX_SEQ_LEN = 128 \n",
    "BATCH_SIZE = 16\n",
    "NONE_LABEL = \"O\" \n",
    "NUM_CLASSES = 3 # IOB (inside, outside, beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dictionaries for other parameters that key off the MODEL_TYPE\n",
    "PRETRAINED_MODEL_NAME = {'biobert': 'bert-base-cased', \n",
    "                         'biomegatron': 'megatron-bert-uncased'}\n",
    "DO_LOWER_CASE = {'biobert': False, \n",
    "               'biomegatron': True}\n",
    "WORK_DIR = {'biobert': '/dli/task/data/logs-ner-biobert/', \n",
    "            'biomegatron': '/dli/task/data/logs-ner-biomegatron'}\n",
    "\n",
    "# Map downloaded ngc models to MODEL_TYPE\n",
    "NGC_CHECKPOINT = {'biobert': CHECKPOINTS_PATH + 'biobert/BERT.pt', \n",
    "                  'biomegatron': CHECKPOINTS_PATH + 'biomegatron/MegatronBERT.pt'}\n",
    "NGC_CONFIG = {'biobert': CHECKPOINTS_PATH + 'biobert/bert_config.json', \n",
    "              'biomegatron': CHECKPOINTS_PATH + 'biomegatron/config.json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Create Neural Modules\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/ner_graph.png\" width=800>\n",
    "    <figcaption style=\"text-align:center;\">NER Graph</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, begin by instantiating the neural module factory before building anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural module factory\n",
    "nf = nemo.core.NeuralModuleFactory(log_dir=WORK_DIR[MODEL_TYPE],\n",
    "                                   create_tb_writer=True,\n",
    "                                   placement=nemo.core.DeviceType.GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation each need a datalayer neural module instantiatted with `BertTokenClassificationDataLayer`.  As a reminder of the inputs required, inspect the signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(BertTokenClassificationDataLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `text_file`, `label_file`, `tokenizer`, `max_seq_length` are all required. The only one of those four items we don't have yet is the tokenizer, so we'll set it up before we set up the data layers.  \n",
    "\n",
    "Of the optional parameters, it looks like the default for pad_label is the one we want already, but shuffle needs to be set to `True` for training, and for convenience we want to use a cache, so `use_cache` needs to be `True`.  We also need to use a different value for `batch_size`. The rest is good to go.\n",
    "\n",
    "In the next cell, the tokenizer and training data layer neural module are instantiated to get you started.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the data Layer neural module for training.\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle and use_cache to True for training \n",
    "USE_CACHE = True\n",
    "tokenizer = get_tokenizer(\n",
    "    tokenizer_name='nemobert',\n",
    "    pretrained_model_name = PRETRAINED_MODEL_NAME[MODEL_TYPE],\n",
    "    do_lower_case=DO_LOWER_CASE[MODEL_TYPE]\n",
    ")\n",
    "dl_train = BertTokenClassificationDataLayer(\n",
    "    text_file=os.path.join(DATA_DIR,'text_train.txt'),\n",
    "    label_file=os.path.join(DATA_DIR,'labels_train.txt'),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_cache=USE_CACHE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next four cells, instantiate:\n",
    "1. Validation data layer neural module\n",
    "1. Language model neural module\n",
    "1. Token classification model neural module\n",
    "1. Loss neural module\n",
    "\n",
    "Look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines.  If you get stuck, look back at the [2.0 NLP Projects with NeMo](020_ExploreNeMo.ipynb) notebook for inspiration or the [solution notebook](solution_notebooks/SOLN_040_NamedEntityRecognition.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate the data Layer neural module for validation.\n",
    "#     Include the input file locations, tokenizer, max_seq_length, and batch size.\n",
    "#     Set the shuffle to False (the default value) and use_cache to True for validation  \n",
    "dl_val = None #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instantiate the Language Model with the get_pretrained_lm_model function\n",
    "#    Since we are defining the checkpoint, include the file locations as values for\n",
    "#    \"config=\", \"pretrained_model_name=\", and \"checkpoint=\"\n",
    "#    Hint: We set these values up earlier with dictionaries keyed off the MODEL_TYPE\n",
    "lm = None #FIXME\n",
    "\n",
    "# Sanity check the number of weight parameters\n",
    "#    It should be 108,310,272 for `bert-base-cased`\n",
    "print(f'{PRETRAINED_MODEL_NAME[MODEL_TYPE]} has {lm.num_weights} weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Instantiate the TokenClassifier\n",
    "#    Include the hidden_size, num_classes (which is 3), num_layers (set to 1), \n",
    "#    and a dropout rate of 0.1\n",
    "lm_hidden_size = lm.hidden_size\n",
    "classifier = None #FIXME\n",
    "\n",
    "# Sanity check the number of weight parameters\n",
    "#    It should be 2307\n",
    "print(f'Classifier has {classifier.num_weights} weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Instantiate the CrossEntropyLossNM Loss Function \n",
    "#  Set the logits_ndim value to 3\n",
    "loss = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job!  Your neural modules are set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Create Neural Graphs\n",
    "\n",
    "Define the neural graphs by linking the output of each neural module with the input of the next one in the pipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 Training Graph\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/ner_train_graph.png\" width=800>\n",
    "    <figcaption style=\"text-align:center;\">Training Graph</figcaption>\n",
    "</figure>\n",
    "As before, we'll use the outputs for each neural module to define the inputs of the next one in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "train_data = dl_train()\n",
    "\n",
    "# Define train_embeddings from the language model, based on inputs from the data layer  \n",
    "train_embeddings = lm(input_ids=train_data.input_ids, token_type_ids=train_data.input_type_ids, attention_mask=train_data.input_mask)\n",
    "\n",
    "# Define the train_logits from the clasifier, based on the embeddings from the language model\n",
    "train_logits = classifier(hidden_states=train_embeddings)\n",
    "\n",
    "# Define the train_loss based on the classifier logits and data layer labels\n",
    "train_loss = loss(logits=train_logits, labels=train_data.labels, loss_mask=train_data.loss_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the `train_data` assignment is slightly more concise than the explicit tuple we used in the text classification notebook. We set `train_data = dl_train()` to create the link in our graph from the data layer and into the language model.  The variable names are defined in NeMo, giving us convenient access, e.g. `train_data.input_ids`, `train_data.input_type_ids`, and so on.  Here's a list from the [`BertTokenClassificationDataLayer` source code](https://github.com/NVIDIA/NeMo/blob/3d6ae0589c1bf0fed2cb038ac80590bebe738e3d/nemo/collections/nlp/nm/data_layers/token_classification_datalayer.py):\n",
    "\n",
    "```python\n",
    "    def output_ports(self):\n",
    "        \"\"\"Returns definitions of module output ports.\n",
    "        input_ids:\n",
    "            indices of tokens which constitute batches of text segments\n",
    "        input_type_ids:\n",
    "            tensor with 0's and 1's to denote the text segment type\n",
    "        input_mask:\n",
    "            bool tensor with 0s in place of tokens to be masked\n",
    "        loss_mask:\n",
    "            used to mask and ignore tokens in the loss function\n",
    "        subtokens_mask:\n",
    "            used to mask all but the first subtoken of the work, could be useful during inference\n",
    "        labels:\n",
    "            token target ids\n",
    "        \"\"\"\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 Exercise: Create a Validation Graph\n",
    "\n",
    "The validation graph is very similar to the training graph. In fact, only the data layer module is different. \n",
    "\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/ner_val_graph.png\" width=800>\n",
    "    <figcaption style=\"text-align:center;\">Validation Graph</figcaption>\n",
    "</figure>\n",
    "\n",
    "In the cell below, look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines (there are four of them).  If you get stuck, look back at the training graph you just set up for inspiration or the [solution notebook](solution_notebooks/SOLN_040_NamedEntityRecognition.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data output from the data layer\n",
    "val_data = None #FIXME\n",
    "\n",
    "# Define val_embeddings from the language model, based on inputs from the data layer  \n",
    "val_embeddings = None #FIXME\n",
    "\n",
    "# Define val_logits from the classifier, based on the embeddings from the language model\n",
    "val_logits = None #FIXME\n",
    "\n",
    "# Define val_loss based on the classifier logits and data layer labels\n",
    "val_loss = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  You're pipelines are ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Training\n",
    "Now that the graphs are set up, the action can begin.  You'll train the model with the NeuralModuleFactory `.train()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 Set the Learning Rate and Optimizer\n",
    "For NER, we'll set the learning rate to 0.00004 and use `WarmupAnnealing`.  We'll use the popular [`adam_w`](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adamw-pytorch) optimizer (with \"weight decay\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "OPTIMIZER = 'adam_w'\n",
    "LEARNING_RATE = 4e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# If you're training on multiple GPUs, this should be\n",
    "# len(train_data_layer) // (batch_size * batches_per_step * num_gpus)\n",
    "steps_per_epoch = len(dl_train) // BATCH_SIZE\n",
    "logging.info(f\"doing {steps_per_epoch} steps per epoch\")\n",
    "lr_policy_fn = get_lr_policy(\"WarmupAnnealing\", \n",
    "                             total_steps=NUM_EPOCHS * steps_per_epoch, \n",
    "                             warmup_ratio=WARMUP_RATIO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 Exercise: Create the Callbacks\n",
    "NeMo has a callback system that can be used to inject user code and logic inside its training loop. You can find references for the newest built-in callbacks in the NeMo Documentation:  \n",
    "\n",
    "* SimpleLogger\n",
    "* TensorboardLogger\n",
    "* CheckpointCallBack\n",
    "\n",
    "In the cell below, create the callbacks, just as we did in the text classification project.  This time, specify a `step_freq=100` value. \n",
    "\n",
    "Look for and fix the <i><strong><span style=\"color:green;\">#FIXME</span><strong></i> code lines.  If you get stuck, look back at the [3.0 Build a 3-Class Text Classifier](030_TextClassification.ipynb) notebook for inspiration or the [solution notebook](solution_notebooks/SOLN_040_NamedEntityRecogntion.ipynb) for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callback = None #FIXME\n",
    "\n",
    "tensorboard_callback = None #FIXME\n",
    "\n",
    "# Callback to evaluate the model\n",
    "eval_callback = EvaluatorCallback(\n",
    "        eval_tensors=[val_logits, val_data.labels, val_data.subtokens_mask],\n",
    "        user_iter_callback=lambda x, y: eval_iter_callback(x, y),\n",
    "        user_epochs_done_callback=lambda x: eval_epochs_done_callback(x, dl_train.dataset.label_ids,\n",
    "                                                                      f'{nf.work_dir}/graphs'),\n",
    "        tb_writer=nf.tb_writer,\n",
    "        eval_step=steps_per_epoch\n",
    "    )\n",
    "\n",
    "# Create callback to save checkpoints\n",
    "ckpt_callback = None #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good!  Let's train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.3 Run the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nemo.core.NeuralModuleFactory.reset_trainer(nf)\n",
    "\n",
    "nf.train(\n",
    "    tensors_to_optimize=[train_loss],\n",
    "    callbacks=[train_callback, tensorboard_callback, eval_callback, ckpt_callback],\n",
    "    lr_policy=lr_policy_fn,\n",
    "    optimizer=OPTIMIZER,\n",
    "    optimization_params={\"num_epochs\": NUM_EPOCHS, \"lr\": LEARNING_RATE, \"weight_decay\": WEIGHT_DECAY}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look something like:\n",
    "```\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:82] Accuracy: 0.9882348032875798\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 weighted: 98.82\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 macro: 93.74\n",
    "[NeMo I 2020-05-22 17:13:48 token_classification_callback:86] F1 micro: 98.82\n",
    "[NeMo I 2020-05-22 17:13:49 token_classification_callback:89] precision    recall  f1-score   support\n",
    "    \n",
    "    O (label id: 0)     0.9938    0.9957    0.9947     22092\n",
    "    B (label id: 1)     0.8843    0.9034    0.8938       787\n",
    "    I (label id: 2)     0.9505    0.8982    0.9236      1090\n",
    "    \n",
    "           accuracy                         0.9882     23969\n",
    "          macro avg     0.9429    0.9324    0.9374     23969\n",
    "       weighted avg     0.9882    0.9882    0.9882     23969\n",
    "```\n",
    "The final confusion matrix visualization shows a bright diagonal, indicating that the predicted label matched the true label with high accuracy for all the label types (IOB).\n",
    "\n",
    "<img src=\"../images/ner_confusion_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Inference\n",
    "We have a test set, which we can read with *pandas*, then sample and export into whatever format we need.  We ultimately want a manageable number of sentences in a list, that we can submit directly to a data layer neural module.  In the text classification example, we provided the data layer with a filename, but this time, we'll just pass a query list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the text data and sanity-check it \n",
    "df_test = pd.read_csv(DATA_DIR + 'text_test.txt', sep='\\t', names=['sentence'])\n",
    "print('The size of the test samples DataFrame: {}'.format(df_test.shape))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a small number (such as 10) of random samples and save them\n",
    "df_test = df_test.sample(10)\n",
    "df_test.shape\n",
    "queries = df_test['sentence'].values.tolist()\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.1 Create the Test Graph\n",
    "<figure>\n",
    "    <img src=\"../images/nemo/ner_test_graph.png\" width=600>\n",
    "    <figcaption style=\"text-align:center;\">Test Graph</figcaption>\n",
    "</figure>\n",
    "\n",
    "We'll use a different data layer neural module called `BertTokenClassificationInferDataLayer`, so that we can use a direct list as input instead of a file (the `queries=` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a data layer neural module for the test set\n",
    "dl_test = BertTokenClassificationInferDataLayer(\n",
    "    tokenizer=tokenizer,\n",
    "    queries=queries,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the neural graph connections\n",
    "test_data = dl_test()\n",
    "test_embeddings = lm(input_ids=test_data.input_ids, \n",
    "                           token_type_ids=test_data.input_type_ids, \n",
    "                           attention_mask=test_data.input_mask)\n",
    "test_logits = classifier(hidden_states=test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 Run Inference on the Test Set\n",
    "Start the action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits_tensors = nf.infer(tensors=[test_logits, test_data.subtokens_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5.3 Inference Results\n",
    "To view the results, we'll gather the resulting output tensors and map them to the words in the `queries` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the results\n",
    "def concatenate(lists):\n",
    "    return np.concatenate([t.cpu() for t in lists])\n",
    "\n",
    "def add_brackets(text, add=True):\n",
    "    return '[' + text + ']' if add else text\n",
    "\n",
    "logits, subtokens_mask = [concatenate(tensors) for tensors in test_logits_tensors]\n",
    "preds = np.argmax(logits, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the queries and display the IOB tags\n",
    "blue_separator = termcolor.colored('----------------------------', color='blue')\n",
    "green_separator = termcolor.colored('*****************************\\n*****************************', color='green')\n",
    "labels_dict = DATA_DIR + \"label_ids.csv\"\n",
    "labels_dict = get_vocab(labels_dict)\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f'Query:\\n{query}')\n",
    "    \n",
    "    pred = preds[i][subtokens_mask[i] > 0.5]\n",
    "    words = query.strip().split()\n",
    "    output = ''\n",
    "    \n",
    "    for j, w in enumerate(words):\n",
    "        output += w\n",
    "        label = labels_dict[pred[j]]\n",
    "        if label != NONE_LABEL:\n",
    "            label = add_brackets(label)\n",
    "            output += label\n",
    "        output += ' '\n",
    "        \n",
    "    print(f'{blue_separator}\\nLabeled Result:\\n{output.strip()}\\n{green_separator}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a series of outputs for each sentence in your query, looking something like the following example.  The labeled result includes an IOB tag where the named entity, in this case a disease, exists.  In this example we see two tags:\n",
    "```\n",
    "ovarian[B] cancer[I] \n",
    "\n",
    "```\n",
    "The \"B\" tag indicates the beginning word of a disease name, and the \"I\" tag indicates an inside word of the disease name.  All other tags are outside the disease name with tag \"O\" not shown. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*****************************\n",
    "*****************************\n",
    "Query:\n",
    "Oral contraceptives protect against ovarian cancer in general , but it is not known whether they also protect against hereditary forms . \n",
    "----------------------------\n",
    "Labeled Result:\n",
    "Oral contraceptives protect against ovarian[B] cancer[I] in general , but it is not known whether they also protect against hereditary forms .\n",
    "*****************************\n",
    "*****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Exercise: Change the Language Model\n",
    "Now that you've built the project, you can experiment with different settings, and try the BioMegatron model.   To do that, you'll need to restart the kernel to clear memory, and change the `MODEL_TYPE` value in the [4.1.1 Input Parameters](#4.1.1-Input-Parameters) section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've mastered NeMo and learned:\n",
    "* How to fine-tune a named entity recognizer\n",
    "* How to apply a domain-specific model from a checkpoint\n",
    "* How to test an NER model with queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
