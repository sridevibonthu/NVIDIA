{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Server Performance\n",
    "\n",
    "In this notebook, you'll implement the optimization techniques you've learned, and profile the resulting model in a more formal way.\n",
    "\n",
    "**[3.1 Assessing the impact of Optimizations](#3.1-Assessing-the-impact-of-Optimizations)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [3.1.1 Exercise: Profile the Model](#3.1.1-Exercise:-Profile-the-Model)<br>\n",
    "**[3.2 Monitoring and Responding to Performance Fluctuations](#3.2-Monitoring-and-Responding-to-Performance-Fluctuations)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [3.2.1 Viewing Prometheus Metrics](#3.2.1-Viewing-Prometheus-Metrics)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [3.2.2 Interpreting the Metrics](#3.2.2-Interpreting-the-Metrics)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll analyze the impact of our configuration changes, as well as how the nature of the request pattern affects our inferencing capability. We will generate structured reports aimed at comparing the performance of a TorchScript-based model with no advanced Triton features activated, to a TensorRT ONNX model with the key features you've learned enabled. \n",
    "\n",
    "We will not only focus on the basic metrics that we have analyzed in the previous parts of the class (throughput and latency), but also try to understand which factors affect the latency of our solution (e.g. network communication).\n",
    "\n",
    "Finally, we will look at the tools that can be used to monitor and manage the performance of our solution in production, and look at how they can be used to implement more advanced functionality like auto-scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Assessing the impact of Optimizations\n",
    "The performance tool that we've been using has an additional feature: not only does it display the results on the screen, it also saves the data in a tabular format to the following location: \n",
    "\n",
    "<code>\"./results/${MODEL_NAME}/results${RESULTS_ID}_${TIMESTAMP}.csv\"</code>\n",
    "\n",
    "To assess the impact of the various optimizations, let's take advantage of the previously generated log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Exercise: Profile the Model\n",
    "We executed <code>bertQA-torchscript</code> as well as <code>bertQA-onnx-trt-dynbatch</code> earlier, so we should already have the logs from that execution saved. Let's look at the content of the appropriate log folders. If you have executed the performance tool more than once, you might see multiple log files with different time stamps created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./results/bertQA-torchscript/\n",
    "!ls ./results/bertQA-onnx-trt-dynbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download both of the CSV files (browse in the left pane and right-click to find \"download\"). In order to generate the execution reports follow the steps below to import the log files of the <code>bertQA-onnx-trt-dynbatch</code>:\n",
    "\n",
    "<!-- - Open [this spreadsheet](Triton%20Inference%20Server%20Performance%20Results.xlsx) -->\n",
    "- Open <a href=\"https://docs.google.com/spreadsheets/d/1S8h0bWBBElHUoLd2SOvQPzZzRiQ55xjyqodm_9ireiw/edit#gid=1572240508\">this spreadsheet</a>\n",
    "- Make a copy from the File menu \"Make a copy…\"\n",
    "- Open the copy\n",
    "- Select the A1 cell on the \"Raw Data\" tab\n",
    "- From the File menu select \"Import…\"\n",
    "- Select \"Upload\" and upload the file\n",
    "- Select \"Replace data at selected cell\" and then select the \"Import data\" button\n",
    "\n",
    "Once you have completed the above steps you should be presented with the following plots in the \"Components of Latency\" tab and \"Latency vs. Throughput\" tab, respectively: <br/>\n",
    "<img width=600 src=\"images/ComponentsOfLatency1.png\"/> <img width=600 src=\"images/LatencyVsThrughput1.png\"/> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please repeat the above for the <code>bertQA-torchscript</code> model. (Remember that the TorchScript variant was executed at batch 8). <br>\n",
    "How do those compare? Discuss with the instructor.\n",
    "\n",
    "Images of the analysis for the `bertQA-torchscript` model can also be found <a href=\"images/torchscript_latency1.png\">here</a> and <a href=\"images/torchscript_latency2.png\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Monitoring and Responding to Performance Fluctuations\n",
    "\n",
    "Understanding the performance of your inference server is not only critical at the initial planning stage but equally important throughout the lifetime of the application. The ability to capture metrics describing server performance is not only central to the ability to respond to issues, but also is a foundation of more advanced features like automatic scaling.  The diagram below demonstrates a simplified view of the Triton deployment architecture. By combining Triton with technologies like [Kubernetes](https://kubernetes.io/docs/home/), you can, with relative ease, create a configuration that will automatically scale with the increased demand within your data center or, if necessary, burst the excess workload to the cloud/clouds. <br/>\n",
    "\n",
    "<img width=700 src=\"images/DeploymentArchitecture.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Viewing Prometheus Metrics\n",
    "Triton exposes [Prometheus](https://prometheus.io/) performance metrics for monitoring on port 8002 by default. These include metrics on GPU power usage, GPU memory, request counts, and latency measures.  More documentation on individual metrics can be found in the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/metrics.html\">Triton Metrics documentation</a>. For now, let's query the metrics captured throughout our performance runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a curl command to request the metrics\n",
    "prometheus_url = tritonServerHostName + \":8002/metrics\"\n",
    "!curl -v {prometheus_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Interpreting the Metrics\n",
    "The Prometheus metrics output is a list of metrics, where each is provided with the form:\n",
    "\n",
    "```\n",
    "# HELP <metric_name and description>\n",
    "# TYPE <metric_name and type>\n",
    "metric_name{gpu_uuid=\"GPU-xxxxxx\",...} <data>\n",
    "```\n",
    "\n",
    "For example, if the inference server models includes two models, you should see among the list some metrics that are specific to each model, and other metrics that are more general about the GPU they both share.<br>\n",
    "\n",
    "#### Count Example\n",
    "The following example indicates that the inference count for the `bertQA-onnx-trt-dynbatch` model is 10,105 so far, while the inference count for `bertQA-torchscript` model is 717.<br>What do your results show?\n",
    "```\n",
    "# HELP nv_inference_count Number of inferences performed\n",
    "# TYPE nv_inference_count counter\n",
    "nv_inference_count{gpu_uuid=\"GPU-640c6e00-43dd-9fae-9f9a-cb6af82df8e9\",model=\"bertQA-onnx-trt-dynbatch\",version=\"1\"} 10105.000000\n",
    "nv_inference_count{gpu_uuid=\"GPU-640c6e00-43dd-9fae-9f9a-cb6af82df8e9\",model=\"bertQA-torchscript\",version=\"1\"} 717.000000\n",
    "```\n",
    "\n",
    "#### GPU Power Example\n",
    "The following example indicates that current GPU power usage is about 40 watts.<br>What do your results show?\n",
    "```\n",
    "# HELP nv_gpu_power_usage GPU power usage in watts\n",
    "# TYPE nv_gpu_power_usage gauge\n",
    "nv_gpu_power_usage{gpu_uuid=\"GPU-640c6e00-43dd-9fae-9f9a-cb6af82df8e9\"} 39.958000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Do Your Results Indicate?\n",
    "\n",
    "* Can you identify the current utilization rate? \n",
    "* Why is it zero? \n",
    "* How much memory are we using? \n",
    "* Why do you think we are using the GPU memory even though there are no requests executed against our server? \n",
    "\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've successfully configured optimizations and learned how to profile the model.<br>\n",
    "\n",
    "Please move to the last part of the class to learn how to build custom applications that take advantage of Triton features:<br>\n",
    "[4.0 Using the Model](040_UsingTheModel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
