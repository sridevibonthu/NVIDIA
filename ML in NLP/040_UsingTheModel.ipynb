{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Using the model\n",
    "\n",
    "In this notebook, you'll \"use\" the server and see real-time inference in action with a question-answering NLP task.\n",
    "\n",
    "**[4.1 The API Basics](#4.1-The-API-Basics)**<br>\n",
    "**[4.2 Inference API Overview](#4.2-Inference-API-Overview)**<br>\n",
    "**[4.3 Preparing the Request](#4.3-Preparing-the-Request)**<br>\n",
    "**[4.4 Querying the Server](#4.4-Querying-the-Server)**<br>\n",
    "**[4.5 Post-Processing the Response](#4.5-Post-Processing-the-Response)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triton Inference Server exposes the services using HTTP and gRPC endpoints. As a consequence, you can query it using a very wide range of tools (e.g. [gRPC](https://grpc.io/docs/languages/) can be used with Java, C++, C# Python, PHP, Ruby, and more).  Triton does not implement its own serving standard; instead it exposes its services using [KFServing Predict protocol version 2](https://github.com/kubeflow/kfserving/tree/master/docs/predict-api/v2). This ensures compatibility with a range of current and future tools that implement serving services.  \n",
    "\n",
    "For further simplification of development, Triton exposes the server protocols through a number of APIs:\n",
    "- [Python API](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/python_api.html?highlight=grpc)\n",
    "- [C++ API](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/cpp_api/cpp_api_root.html)\n",
    "- [Protobuf API](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/protobuf_api/protobuf_api_root.html)\n",
    "\n",
    "In this example, you will learn how to consume our question-answering service using the Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 The API Basics\n",
    "\n",
    "Let us start by reviewing the basic components of the API. The key element is the <code>tritonhttpclient</code> which can be obtained either by downloading the NGC container with the <a href=\"https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver\">Triton client utilities</a>, or by downloading it directly from the <a href=\"https://github.com/NVIDIA/triton-inference-server/releases/tag/v2.0.0\">Triton GitHub Repository</a>. Once deployed (they are already installed in this class), import the appropriate library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tritonhttpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the client by pointing it towards our server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"triton:8000\", verbose=True)\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, inspect the status of our server, and availability and status of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-torchscript\"\n",
    "print(triton_client.is_server_live())\n",
    "print(triton_client.is_server_ready())\n",
    "print(triton_client.is_model_ready(modelName,\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, inspect the metadata returned by the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.get_server_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to basic health checks and model inference, the API provides fine-grained control over the server, enabling actions such as loading and unloading models. For more information, please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/python_api.html\">documentation</a> and the <a href=\"https://github.com/NVIDIA/triton-inference-server/tree/60c33d5593ad0d50716f04f69bb4b24ee3a7996d/src/clients/python/examples\">API examples</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Inference API Overview\n",
    "\n",
    "Since we have been working with a neural network built to do question answering, we'll run an example query against our server. To start, let's investigate the shape of the input and output data that the server will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_client.get_model_metadata(modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have recieved a response similar to the below: <br/>\n",
    "<img width=1000 src=\"images/DataFormat.png\"/>\n",
    "\n",
    "The server indicated that it expects three input tensors:\n",
    "- input__0 being the input_ids\n",
    "- input_1 being the sequence_ids\n",
    "- input_2 being the mask_ids\n",
    "\n",
    "The server will respond with:\n",
    "- output__0 being the start logits\n",
    "- output_-1 being the end logits\n",
    "\n",
    "We now need to pre process our question and context into the format required by the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Preparing the Request\n",
    "\n",
    "Start by creating the question and an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Most antibiotics target bacteria and don't affect what class of organisms? \"\n",
    "context = \"Within the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by \" +\\\n",
    "        \"competing with pathogenic bacteria for food and space and, in some cases, by changing the conditions in \" +\\\n",
    "        \"their environment, such as pH or available iron. This reduces the probability that pathogens will \" +\\\n",
    "        \"reach sufficient numbers to cause illness. However, since most antibiotics non-specifically target bacteria\" +\\\n",
    "        \"and do not affect fungi, oral antibiotics can lead to an overgrowth of fungi and cause conditions such as a\" +\\\n",
    "        \"vaginal candidiasis (a yeast infection). There is good evidence that re-introduction of probiotic flora, such \" +\\\n",
    "        \"as pure cultures of the lactobacilli normally found in unpasteurized yogurt, helps restore a healthy balance of\" +\\\n",
    "        \"microbial populations in intestinal infections in children and encouraging preliminary data in studies on bacterial \" +\\\n",
    "        \"gastroenteritis, inflammatory bowel diseases, urinary tract infection and post-surgical infections. \" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, by importing some additional utilities that will hide the boilerplate logic necessary for data transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/dli/task/client')\n",
    "from tokenization import BertTokenizer\n",
    "from inference import preprocess_tokenized_text,parse_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code transforms the data into the required format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(\"/dli/task/vocab\", do_lower_case=True, max_len=512) \n",
    "doc_tokens = context.split()\n",
    "query_tokens = tokenizer.tokenize(question)\n",
    "\n",
    "tensors_for_inference, tokens_for_postprocessing = preprocess_tokenized_text(doc_tokens, \n",
    "                                    query_tokens, \n",
    "                                    tokenizer, \n",
    "                                    max_seq_length=384, \n",
    "                                    max_query_length=64)\n",
    "\n",
    "dtype = np.int64\n",
    "input_ids = np.array(tensors_for_inference.input_ids, dtype=dtype)[None,...] # make bs=1\n",
    "segment_ids = np.array(tensors_for_inference.segment_ids, dtype=dtype)[None,...] # make bs=1\n",
    "input_mask = np.array(tensors_for_inference.input_mask, dtype=dtype)[None,...] # make bs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we copy the data into the structures required by Triton. Do notice that we use tensor names, data types and tensor dimensions as specified by the Triton server response earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "inputs.append(tritonhttpclient.InferInput('input__0', [1, len(input_ids[0])], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput('input__1', [1, len(segment_ids[0])], \"INT64\"))\n",
    "inputs.append(tritonhttpclient.InferInput('input__2', [1, len(input_mask[0])], \"INT64\"))\n",
    "\n",
    "\n",
    "inputs[0].set_data_from_numpy(input_ids, binary_data=False)\n",
    "inputs[1].set_data_from_numpy(segment_ids, binary_data=False)\n",
    "inputs[2].set_data_from_numpy(input_mask, binary_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting one of the inputs reveals the new data representation, which was tokenized and converted to the numerical format as required by the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs[0]._get_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it is possible to just fetch all of the output tensors associated with the request it is a good practice to fetch only the bare minimum to minimize the bandwidth. We do that by specifying the request output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "outputs.append(\n",
    "        tritonhttpclient.InferRequestedOutput('output__0', binary_data=False))\n",
    "outputs.append(\n",
    "        tritonhttpclient.InferRequestedOutput('output__1', binary_data=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Querying the Server\n",
    "\n",
    "Let us now issue a request to the server. The <code>outputs</code> parameter is optional. If not specified all tensors will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = triton_client.infer(modelName,\n",
    "                                  inputs,\n",
    "                                  outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the <code>results</code> and <code>outputs</code> are of the same data type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Post-Processing the Response\n",
    "\n",
    "The results in our case are just logits of start and end positions. Let's process those further to obtain a human readable result. We start by copying the vectors to NumPy to make further processing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the results by comparing with precomputed values.\n",
    "output0_data = results.as_numpy('output__0')\n",
    "output1_data = results.as_numpy('output__1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output0_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and convert it into a human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = output0_data[0].tolist()\n",
    "end_logits = output1_data[0].tolist()\n",
    "\n",
    "answer, answers = parse_answer(doc_tokens, tokens_for_postprocessing, \n",
    "                                 start_logits, end_logits)\n",
    "\n",
    "# print result\n",
    "print()\n",
    "print(answer)\n",
    "print()\n",
    "print(json.dumps(answers, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ... TA-DA!  We have our results! Feel free to experiment with your own queries.\n",
    "\n",
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've completed the course! \n",
    "\n",
    "Please make sure you fill in the course survey and consider doing the student assessment to obtain a certificate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
