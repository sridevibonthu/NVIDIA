{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 NLP Projects with NVIDIA NeMo\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/nemo/nemo-app-stack.png\" width=350>\n",
    "\n",
    "\n",
    "In this notebook, you'll learn the basics of setting up an NLP project with [NVIDIA NeMo](https://developer.nvidia.com/nvidia-nemo).  You'll learn what components are needed, where to find them, and how to instantiate them.  \n",
    "\n",
    "**[2.1 Programming Model](#2.1-Programming-Model)**<br>\n",
    "**[2.2 Create Neural Modules (Step 1)](#2.2-Create-Neural-Modules-(Step-1))**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 Data Layer and Tokenizer](#2.2.1-Data-Layer-and-Tokenizer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2 Language Model and Classifier](#2.2.2-Language-Model-and-Classifier)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.3 Loss Function](#2.2.3-Loss-Function)<br>\n",
    "**[2.3 Create Neural Graph (Step 2)](#2.3-Create-Neural-Graph-(Step-2))**<br>\n",
    "**[2.4 Start an Action (Step 3)](#2.4-Start-an-Action-(Step-3))**<br>\n",
    "\n",
    "NeMo is an open source, high-performance toolkit for building conversational AI applications using a PyTorch backend.  NeMo provides a level of abstraction beyond Keras or PyTorch, that makes it possible to easily compose complex neural network architectures using reusable components called _neural modules_ (hence the name NeMo). NeMo includes capabilities to scale training to multi-GPU systems and multi-node clusters as well, and provides a straightforward path to model deployment for production real-time inference. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Programming Model\n",
    "NeMo includes library collections for all aspects of the conversational AI pipeline: speech recognition (`nemo_asr`), natural language (`nemo_nlp`), and speech synthesis (`nemo_tts`).  For our projects, we'll use the `nemo-nlp` and some core libraries.\n",
    "\n",
    "Building an application using NeMo APIs consists of three basic steps:\n",
    "1. **Create neural modules** after instantiating the NeuralModuleFactory:<br>\n",
    "A _NeuralModule_ is an abstraction between a network layer and a full neural network, for example: encoder, decoder, language model, acoustic model, etc.  \n",
    "1. **Create the neural graph** of tensors connecting the neural modules together:<br>\n",
    "Define all the inputs, outputs, and connections you need to form a pipeline.\n",
    "1. **Start an \"action\"** such as `train` for training a network or `infer` obtain a result from a network:<br>\n",
    "We can also define _callbacks_ for evaluation, logging and performance monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the nemo and nlp imports (Ignore the torchaudio error if it appears).\n",
    "# Import \"inspect\" so we can look at method signatures.\n",
    "import nemo\n",
    "import nemo.collections.nlp as nemo_nlp\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Create Neural Modules (Step 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating any neural modules, we must instantiate the `NeuralModuleFactory` object.  There are a number of settings that we'll use in the projects to specify directories for intermediate information such as checkpoints and logs.  This is how to instantiate the factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the NeuralModuleFactory before creating neural modules.\n",
    "nf = nemo.core.NeuralModuleFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Modules can be conceptually classified into four potentially overlapping categories:\n",
    "\n",
    "* Data Layers - modules that perform extract, transform, load, and feed of the data\n",
    "* Trainable Modules - modules that contain trainable weights\n",
    "* Non Trainable Modules - non-trainable module, for example, table lookup, data augmentation, greedy decoder, etc. \n",
    "* Loss Modules - modules that compute loss functions\n",
    "\n",
    "For text classification and NER, we will need a data layer, a trainable language model, a trainable classifier, and a loss module (shown as boxes in the diagram below).  \n",
    "\n",
    "<img src=\"images/nemo/nm-pipe.png\" width=800>\n",
    "     \n",
    "It is possible to build your own neural modules, but there are plenty available for us to use already.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Data Layer and Tokenizer\n",
    "Run the following cell to see a list of available NLP data layer neural modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertInferDataLayer',\n",
       " 'BertJointIntentSlotDataLayer',\n",
       " 'BertJointIntentSlotInferDataLayer',\n",
       " 'BertPretrainingDataLayer',\n",
       " 'BertPretrainingPreprocessedDataLayer',\n",
       " 'BertQuestionAnsweringDataLayer',\n",
       " 'BertTextClassificationDataLayer',\n",
       " 'BertTokenClassificationDataLayer',\n",
       " 'BertTokenClassificationInferDataLayer',\n",
       " 'GlueClassificationDataLayer',\n",
       " 'GlueRegressionDataLayer',\n",
       " 'LanguageModelingDataLayer',\n",
       " 'LaserTaggerDataLayer',\n",
       " 'MultiWOZDataLayer',\n",
       " 'PunctuationCapitalizationDataLayer',\n",
       " 'SGDDataLayer',\n",
       " 'TextDataLayer',\n",
       " 'TranslationDataLayer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the callable data layer objects.\n",
    "[method_name for method_name in dir(nemo_nlp.nm.data_layers)\n",
    "                  if callable(getattr(nemo_nlp.nm.data_layers, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification project, we'll use the `BertTextClassificationDataLayer` neural module, while for NER we'll use `BertTokenClassificationDataLayer`.  Take a look at the class signatures below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (input_file, tokenizer, max_seq_length, num_samples=-1, shuffle=False, batch_size=64, use_cache=False, dataset_type=<class 'nemo.collections.nlp.data.datasets.text_classification.text_classification_dataset.BertTextClassificationDataset'>)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(nemo_nlp.nm.data_layers.BertTextClassificationDataLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (text_file, label_file, tokenizer, max_seq_length, pad_label='O', label_ids=None, num_samples=-1, shuffle=False, batch_size=64, ignore_extra_tokens=False, ignore_start_end=False, use_cache=False, dataset_type=<class 'nemo.collections.nlp.data.datasets.token_classification_dataset.BertTokenClassificationDataset'>)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(nemo_nlp.nm.data_layers.BertTokenClassificationDataLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it has a few differences, such as the imported `text_file` and `label_file` parameters needed for NER, rather than the single `input_file` used for the text classification task.\n",
    "\n",
    "In both cases, we need to also provide the `tokenizer` and `max_seq_length` parameters.  For the maximum sequence length, we have a tradeoff here between providing enough information for the system to employ useful attention, while maintaining efficiency of memory and processing time.  The maximum size for BERT is 512. Depending on the particular task and constraints, we might typically choose 32, 64, 128, 256, or 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "A _tokenizer_ segments text and documents into a given type of token, which may be sentences, sentence pieces, words, word pieces, or even characters.  Take a look at the choices currently available within the NeMo library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CharTokenizer',\n",
       " 'NemoBertTokenizer',\n",
       " 'NemoGPT2Tokenizer',\n",
       " 'SentencePieceTokenizer',\n",
       " 'WordTokenizer',\n",
       " 'YouTokenToMeTokenizer',\n",
       " 'get_bert_special_tokens',\n",
       " 'get_tokenizer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the callable tokenizer objects.\n",
    "[method_name for method_name in dir(nemo_nlp.data.tokenizers)\n",
    "                  if callable(getattr(nemo_nlp.data.tokenizers, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll be starting with pretrained BERT models and fine-tuning from there, our tokenizer must use the one that the BERT model expects, which happens to be the `NemoBertTokenizer`.  In addition to breaking apart the input text, the tokenizer converts each token to an _id_ associated with a vocabulary file.  This vocabulary can either be provided by the user, or must be calculated when the tokenizer is specified, which may take a bit of time.  \n",
    "\n",
    "The details of the tokenizer instantiation can be abstracted somewhat by using the `get_tokenizer` helper.  This is a little easier in practice since it abstracts some details related to exactly which BERT or Megatron pretrained model we choose.  For our example, we'll use `bert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-06-08 15:51:29 bert_tokenizer:78] Deriving bert model type from pretrained model name.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355d3d82a63c420c82686bcc8a7cc168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nemo.collections.nlp.data.tokenizers.get_tokenizer(tokenizer_name='nemobert', \n",
    "                                                               pretrained_model_name='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT tokenizer uses _wordpiece_ tokenization, and since we have specified an uncased model, we would expect the tokenizer to turn all the words to lower case as well.  Run the following cell to see how the tokenizer breaks apart a sentence with a wordpiece algorithm, and encodes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How about this stupendous sentence?\n",
      "['how', 'about', 'this', 'stu', '##pen', '##dou', '##s', 'sentence', '?']\n",
      "[2129, 2055, 2023, 24646, 11837, 26797, 2015, 6251, 1029]\n"
     ]
    }
   ],
   "source": [
    "# Show tokens from an input\n",
    "test_sentence = 'How about this stupendous sentence?'\n",
    "tokenized = tokenizer.text_to_tokens(test_sentence)\n",
    "encoded = tokenizer.text_to_ids(test_sentence)\n",
    "print('{}\\n{}\\n{}'.format(test_sentence, tokenized, encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate a nominal data layer neural module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-06-08 15:53:01 text_classification_dataset:82] data/NCBI_tc-3/dev.tsv: 100\n",
      "[NeMo I 2021-06-08 15:53:03 data_preprocessing:250] Min: 120 |                  Max: 506 |                  Mean: 307.0 |                  Median: 301.5\n",
      "[NeMo I 2021-06-08 15:53:03 data_preprocessing:252] 75 percentile: 363.0\n",
      "[NeMo I 2021-06-08 15:53:03 data_preprocessing:253] 99 percentile: 503.03\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:118] 99 out of 100                         sentences with more than 128 subtokens.\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:175] *** Example ***\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:176] example_index: 0\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:177] subtokens: [CLS] remained elusive . we now show that br ##ca ##1 en ##codes a 190 - k ##d protein with sequence homo ##logy and bio ##chemical analogy to the gran ##in protein family . interesting ##ly , br ##ca ##2 also includes a motif similar to the gran ##in consensus at the c terminus of the protein . both br ##ca ##1 and the gran ##ins local ##ize to secret ##ory ve ##sic ##les , are secret ##ed by a regulated pathway , are post - translation ##ally g ##ly ##cos ##yla ##ted and are responsive to hormones . as a regulated secret ##ory protein , br ##ca ##1 appears to function by a mechanism not previously described for tu ##mour suppress ##or gene products . . [SEP]\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:178] sent_label: 0\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:179] input_ids: 101 2815 26475 1012 2057 2085 2265 2008 7987 3540 2487 4372 23237 1037 11827 1011 1047 2094 5250 2007 5537 24004 6483 1998 16012 15869 23323 2000 1996 12604 2378 5250 2155 1012 5875 2135 1010 7987 3540 2475 2036 2950 1037 16226 2714 2000 1996 12604 2378 10465 2012 1996 1039 7342 1997 1996 5250 1012 2119 7987 3540 2487 1998 1996 12604 7076 2334 4697 2000 3595 10253 2310 19570 4244 1010 2024 3595 2098 2011 1037 12222 12732 1010 2024 2695 1011 5449 3973 1043 2135 13186 23943 3064 1998 2024 26651 2000 20752 1012 2004 1037 12222 3595 10253 5250 1010 7987 3540 2487 3544 2000 3853 2011 1037 7337 2025 3130 2649 2005 10722 20360 16081 2953 4962 3688 1012 1012 102\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:180] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:175] *** Example ***\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:176] example_index: 1\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:177] subtokens: [CLS] ##var ##ian cancer was 2 . 11 times greater for br ##ca ##1 carriers harbour ##ing one or two rare hr ##as ##1 all ##eles , compared to carriers with only common all ##eles ( p = 0 . 01 ##5 ) . the magnitude of the relative risk associated with a rare hr ##as ##1 all ##ele was not altered by adjusting for the other known risk factors for hereditary o ##var ##ian cancer ( 5 ) . su ##sc ##ept ##ibility to breast cancer did not appear to be affected by the presence of rare hr ##as ##1 all ##eles . this study is the first to show the effect of a modifying gene on the pen ##et ##rance of an inherited cancer syndrome [SEP]\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:178] sent_label: 0\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:179] input_ids: 101 10755 2937 4456 2001 1016 1012 2340 2335 3618 2005 7987 3540 2487 11363 7440 2075 2028 2030 2048 4678 17850 3022 2487 2035 26741 1010 4102 2000 11363 2007 2069 2691 2035 26741 1006 1052 1027 1014 1012 5890 2629 1007 1012 1996 10194 1997 1996 5816 3891 3378 2007 1037 4678 17850 3022 2487 2035 12260 2001 2025 8776 2011 19158 2005 1996 2060 2124 3891 5876 2005 14800 1051 10755 2937 4456 1006 1019 1007 1012 10514 11020 23606 13464 2000 7388 4456 2106 2025 3711 2000 2022 5360 2011 1996 3739 1997 4678 17850 3022 2487 2035 26741 1012 2023 2817 2003 1996 2034 2000 2265 1996 3466 1997 1037 29226 4962 2006 1996 7279 3388 21621 1997 2019 7900 4456 8715 102\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:180] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:175] *** Example ***\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:176] example_index: 2\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:177] subtokens: [CLS] island at the 3 end of d ##mp ##k . sequencing of this region shows that the island extends over 3 . 5 kb and is interrupted by the ( ct ##g ) n repeat . comparison of gen ##omic sequences downstream ( centro ##meric ) of the repeat in human and mouse identified regions of significant homo ##logy . these correspond to ex ##ons of a gene predicted to en ##code a home ##od ##oma ##in protein . rt - pc ##r analysis shows that this gene , which we have called d ##m locus - associated home ##od ##oma ##in protein ( d ##mah ##p ) , is expressed in a number of human tissues , including skeletal muscle , heart and brain . [SEP]\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:178] sent_label: 1\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:179] input_ids: 101 2479 2012 1996 1017 2203 1997 1040 8737 2243 1012 24558 1997 2023 2555 3065 2008 1996 2479 8908 2058 1017 1012 1019 21677 1998 2003 7153 2011 1996 1006 14931 2290 1007 1050 9377 1012 7831 1997 8991 22026 10071 13248 1006 18120 25531 1007 1997 1996 9377 1999 2529 1998 8000 4453 4655 1997 3278 24004 6483 1012 2122 17254 2000 4654 5644 1997 1037 4962 10173 2000 4372 16044 1037 2188 7716 9626 2378 5250 1012 19387 1011 7473 2099 4106 3065 2008 2023 4962 1010 2029 2057 2031 2170 1040 2213 25206 1011 3378 2188 7716 9626 2378 5250 1006 1040 25687 2361 1007 1010 2003 5228 1999 1037 2193 1997 2529 14095 1010 2164 20415 6740 1010 2540 1998 4167 1012 102\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:180] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:175] *** Example ***\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:176] example_index: 3\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:177] subtokens: [CLS] , which allow for amp ##li ##fication of the coding and sp ##lic ##ing sequences only . the positioning of the polymer ##ase chain reaction ( pc ##r ) prime ##rs was such that the resulting pc ##r products were of different sizes , which enabled us to analyze two different ex ##ons simultaneously and still distinguish between the band ##ing profiles for both ( bi ##plex analysis ) . by using this approach , we were able to identify mutation in 22 new patients , but the overall efficiency of the procedure when we used a single - pass regime ##n was only 48 % . the mutations were small insertion ##s and del ##eti ##ons and point mutations in roughly equal proportions . . [SEP]\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:178] sent_label: 2\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:179] input_ids: 101 1010 2029 3499 2005 23713 3669 10803 1997 1996 16861 1998 11867 10415 2075 10071 2069 1012 1996 19120 1997 1996 17782 11022 4677 4668 1006 7473 2099 1007 3539 2869 2001 2107 2008 1996 4525 7473 2099 3688 2020 1997 2367 10826 1010 2029 9124 2149 2000 17908 2048 2367 4654 5644 7453 1998 2145 10782 2090 1996 2316 2075 17879 2005 2119 1006 12170 19386 4106 1007 1012 2011 2478 2023 3921 1010 2057 2020 2583 2000 6709 16221 1999 2570 2047 5022 1010 2021 1996 3452 8122 1997 1996 7709 2043 2057 2109 1037 2309 1011 3413 6939 2078 2001 2069 4466 1003 1012 1996 14494 2020 2235 23851 2015 1998 3972 20624 5644 1998 2391 14494 1999 5560 5020 19173 1012 1012 102\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:180] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:175] *** Example ***\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:176] example_index: 4\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:177] subtokens: [CLS] ph ##e ) of the c2 ##q ##0 gene linked to the m ##hc ha ##pl ##otype a1 ##1 , b ##35 , dr ##w ##1 , bf ##s , c ##4 ##a ##0 ##b ##1 . the other is in ex ##on 11 ( g ##19 ##30 - - > a ; g ##ly ##44 ##4 - - > ar ##g ) of the c2 ##q ##0 gene linked to the m ##hc ha ##pl ##otype a2 , b ##5 , dr ##w ##4 , bf ##s , c ##4 ##a ##3 ##b ##1 . each mutant c2 gene product is retained early in the secret ##ory pathway . these mutants provide models for el ##uc ##ida ##ting the c2 secret ##ory pathway . . [SEP]\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:178] sent_label: 2\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:179] input_ids: 101 6887 2063 1007 1997 1996 29248 4160 2692 4962 5799 2000 1996 1049 16257 5292 24759 26305 17350 2487 1010 1038 19481 1010 2852 2860 2487 1010 28939 2015 1010 1039 2549 2050 2692 2497 2487 1012 1996 2060 2003 1999 4654 2239 2340 1006 1043 16147 14142 1011 1011 1028 1037 1025 1043 2135 22932 2549 1011 1011 1028 12098 2290 1007 1997 1996 29248 4160 2692 4962 5799 2000 1996 1049 16257 5292 24759 26305 22441 1010 1038 2629 1010 2852 2860 2549 1010 28939 2015 1010 1039 2549 2050 2509 2497 2487 1012 2169 15527 29248 4962 4031 2003 6025 2220 1999 1996 3595 10253 12732 1012 2122 23892 3073 4275 2005 3449 14194 8524 3436 1996 29248 3595 10253 12732 1012 1012 102\n",
      "[NeMo I 2021-06-08 15:53:03 text_classification_dataset:180] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "dl = nemo_nlp.nm.data_layers.BertTextClassificationDataLayer(input_file='data/NCBI_tc-3/dev.tsv', \n",
    "                                                             tokenizer=tokenizer, \n",
    "                                                             max_seq_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Language Model and Classifier\n",
    "Run the following cell to see a list of available NLP trainable neural modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albert',\n",
       " 'BERT',\n",
       " 'BeamSearchTranslatorNM',\n",
       " 'BertTokenClassifier',\n",
       " 'EncoderRNN',\n",
       " 'GreedyLanguageGeneratorNM',\n",
       " 'JointIntentSlotClassifier',\n",
       " 'PunctCapitTokenClassifier',\n",
       " 'Roberta',\n",
       " 'SGDDecoderNM',\n",
       " 'SGDEncoderNM',\n",
       " 'SequenceClassifier',\n",
       " 'SequenceRegression',\n",
       " 'TRADEGenerator',\n",
       " 'TokenClassifier',\n",
       " 'TransformerDecoderNM',\n",
       " 'TransformerEncoderNM',\n",
       " 'get_huggingface_lm_model',\n",
       " 'get_huggingface_lm_models_list',\n",
       " 'get_megatron_checkpoint',\n",
       " 'get_megatron_config_file',\n",
       " 'get_megatron_lm_models_list',\n",
       " 'get_megatron_vocab_file',\n",
       " 'get_pretrained_lm_model',\n",
       " 'get_pretrained_lm_models_list']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the callable trainable neural module objects\n",
    "[method_name for method_name in dir(nemo_nlp.nm.trainables)\n",
    "                  if callable(getattr(nemo_nlp.nm.trainables, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "We'll use the `get_pretrained_lm_model` method to specify a language model.  We can see a list of available models for us to try.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nemo_nlp.nm.trainables.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to instantiate this type of neural module.  Recall that we already chose 'bert-base-uncased' for the tokenizer. The language model chosen here should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336e9a531d124bf99126b0ee7fbbb724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2170d349aaf24cd7980ec97e06f114d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the language model.\n",
    "lm = nemo_nlp.nm.trainables.get_pretrained_lm_model('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "For text classification we'll need the `SequenceClassifier` trainable neural module, while the NER project will require the similar `TokenClassifier` neural module.  Take a look at the signatures below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (hidden_size, num_classes, num_layers=2, activation='relu', log_softmax=True, dropout=0.0, use_transformer_pretrained=True)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(nemo_nlp.nm.trainables.SequenceClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (hidden_size, num_classes, name=None, num_layers=2, activation='relu', log_softmax=True, dropout=0.0, use_transformer_pretrained=True)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(nemo_nlp.nm.trainables.TokenClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, we must provide the `hidden_size`, which we can pull from the language model, and the number of label categories in `num_classes`.  We may wish to alter the other default values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classification neural module\n",
    "cl = nemo_nlp.nm.trainables.SequenceClassifier(hidden_size=lm.hidden_size, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Loss Function\n",
    "Finally, we define the loss function neural module.  For both of our projects, we will use `CrossEntropyLossNM`.  Here's a list of what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BCEWithLogitsLossNM',\n",
       " 'CrossEntropyLossNM',\n",
       " 'LabelsType',\n",
       " 'LogitsType',\n",
       " 'LossAggregatorNM',\n",
       " 'LossNM',\n",
       " 'LossType',\n",
       " 'MSELoss',\n",
       " 'MaskType',\n",
       " 'NeuralType',\n",
       " 'RegressionValuesType',\n",
       " 'SequenceLoss',\n",
       " 'add_port_docs']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the callable loss common neural module objects\n",
    "[method_name for method_name in dir(nemo.backends.pytorch.common.losses)\n",
    "                  if callable(getattr(nemo.backends.pytorch.common.losses, method_name)) \n",
    "                     and method_name[0] not in ['_']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (logits_ndim=2, weight=None, reduction='mean')>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(nemo.backends.pytorch.common.losses.CrossEntropyLossNM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the loss function neural module\n",
    "lf = nemo.backends.pytorch.common.losses.CrossEntropyLossNM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Create Neural Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are four modules in place, defining the graph is just a matter of connecting each of the modules.  We'll define the output of each as the input of the next in the pipeline.\n",
    "\n",
    "To set up the data link, first get the outputs from the data layer. \n",
    "\n",
    "<img src=\"images/nemo/data-tensors.png\" width=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, token_types, attn_mask, labels = dl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine the embeddings, or outputs from the language model, based on the data inputs provided by the data layer.\n",
    "\n",
    "<img src=\"images/nemo/embedding-tensors.png\" width=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = lm(input_ids=input_data, token_type_ids=token_types, attention_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion, define the logits (the raw, non-normalized prediction tensors) generated by the classifier.  The loss function outputs are dependent upon both the logits and the labels available from the data layer.\n",
    "\n",
    "<img src=\"images/nemo/logits-tensors.png\" width=320>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cl(hidden_states=embeddings)\n",
    "loss=lf(logits=logits, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  Now that a graph is defined and ready for action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Start an Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions of training and inference are run from the `NeuralModuleFactory` that you instantiated at the start of the exercise.  Though we don't need to actually execute the actions in this notebook, take a look at the method signatures to see what kind of information we'll need to provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (self, tensors_to_optimize=None, training_graph=None, optimizer=None, optimization_params=None, callbacks:Union[List[Union[nemo.core.deprecated_callbacks.ActionCallback, nemo.core.callbacks.NeMoCallback]], NoneType]=None, lr_policy=None, batches_per_step=None, stop_on_nan_loss=False, synced_batchnorm=False, synced_batchnorm_groupsize=0, gradient_predivide=False, amp_max_loss_scale=16777216.0, reset=False)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(nemo.core.NeuralModuleFactory.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (self, tensors:List[nemo.core.neural_types.neural_type.NmTensor], checkpoint_dir=None, ckpt_pattern='', verbose=True, cache=False, use_cache=False, offload_to_cpu=True, modules_to_restore=None)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(nemo.core.NeuralModuleFactory.infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the setup for training in the project notebooks, you'll need to specify some functions for optimization, learning rate policy, and callbacks. The callbacks are invoked during the processing for purposes or recording intermediate parameters.  This is useful for saving checkpoints and monitoring results. The signatures can be viewed below, and actual use examples are provided in the project notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.SimpleLogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.TensorboardLogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.EvaluatorCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(nemo.core.CheckpointCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've explored the NeMo API and learned:\n",
    "* The three parts of a project in NeMo are: neural modules, neural graphs, and an action\n",
    "* Neural modules we need for these projects are data layers, language models, classifiers, and loss functions\n",
    "* Neural graphs are created by defining the inputs and outputs between the neural modules\n",
    "* Actions are `train` and `infer`\n",
    "\n",
    "You are ready to build the text classification project.<br>\n",
    "\n",
    "Move on to [3.0 Build a 3-class Text Classifier](030_TextClassification.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
